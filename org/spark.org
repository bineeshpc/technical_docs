* sql join in diagramatic form
#+BEGIN_SRC sh
xdg-open sqljoin.png
firefox https://www.google.co.in/search?q=sql+join+venn+diagram&client=ubuntu&hs=cPe&channel=fs&biw=1535&bih=805&tbm=isch&imgil=SXGHjyoV2uc_DM%253A%253Bwgr8RIHzcKDOHM%253Bhttp%25253A%25252F%25252Fwww.codeproject.com%25252FArticles%25252F33052%25252FVisual-Representation-of-SQL-Joins&source=iu&pf=m&fir=SXGHjyoV2uc_DM%253A%252Cwgr8RIHzcKDOHM%252C_&usg=__MfZs8aa97jg__Y52Io4c6KOhYUc%3D&ved=0ahUKEwiZ3ePolvPPAhWLs48KHakxBGgQyjcINA&ei=JNsNWJm7G4vnvgSp45DABg#imgrc=mogSTbjuV7hmkM%3A
#+END_SRC

#+RESULTS:

* Spark overview
http://spark.apache.org/docs/latest/sql-programming-guide.html
Overview

Spark SQL is a Spark module for structured data processing.  
Methods to interact with Spark SQL a) SQL b) Dataset API.
* DataFrame
A DataFrame is a Dataset organized into named columns.

Json and parquet files
DataFrame can write to parquet files
* Example codes of untyped dataset operations or DataFrame operations
#+BEGIN_SRC sh :results output
cat /home/bineesh/temp/hadoop/spark-2.0.0-bin-hadoop2.7/examples/src/main/resources/people.json
#+END_SRC

#+RESULTS:
: {"name":"Michael"}
: {"name":"Andy", "age":30}
: {"name":"Justin", "age":19}

#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("dummy app name").setMaster("local")
sc = SparkContext(conf=conf)
from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

path = '/home/bineesh/temp/hadoop/spark-2.0.0-bin-hadoop2.7/examples/src/main/resources/people.json'
df = sqlContext.read.json(path)


# Displays the content of the DataFrame to stdout
df.show()
print "Schema is"
df.printSchema()

print "only name column"
df.select("name").show()

print "df name and df age + 1"
df.select(df['name'], df['age'] + 1).show()


print "Select people older than 21"
df.filter(df['age'] > 21).show()
## age name
## 30  Andy


print "Count people by age"
df.groupBy("age").count().show()
## age  count
## null 1
## 19   1
## 30   1
"""
"""
#+END_SRC

#+RESULTS:
#+begin_example
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

Schema is
root
 |-- age: long (nullable = true)
 |-- name: string (nullable = true)

only name column
+-------+
|   name|
+-------+
|Michael|
|   Andy|
| Justin|
+-------+

df name and df age + 1
+-------+---------+
|   name|(age + 1)|
+-------+---------+
|Michael|     null|
|   Andy|       31|
| Justin|       20|
+-------+---------+

Select people older than 21
+---+----+
|age|name|
+---+----+
| 30|Andy|
+---+----+

Count people by age
+----+-----+
| age|count|
+----+-----+
|  19|    1|
|null|    1|
|  30|    1|
+----+-----+

#+end_example

Interesting observation.
Because of lazy evaluation I was unable to delete the 
temporary file I created.
Only when the show command is called the evaluation of the dataframe
happened
#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("dummy app name").setMaster("local")
sc = SparkContext(conf=conf)
from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

from tempfile import NamedTemporaryFile

raw_data = """
{"name":"Michael"}
{"name":"Andy", "age":30}
{"name":"Justin", "age":19}
{"name":"Albert", "age":100}
{"name":"Niel", "age":100}
{"name":"Vargheese", "age":30}
"""

def read_from_string(string1):
    with NamedTemporaryFile(delete=False) as f:
        f.write(raw_data)
        tempfilename = f.name
    df = sqlContext.read.json(tempfilename)
    return df    

df = read_from_string(raw_data)
print "Count people by age"
df.groupBy("age").count().show()

df.createOrReplaceTempView("people")

print "Printing everything from table using sql"
sqlDF = sqlContext.sql("SELECT * FROM people")
sqlDF.show()

print "selectting columns based on a filter using sql"
sqlContext.sql("select * from people where age = 30").show()

#+END_SRC

#+RESULTS:
#+begin_example
Count people by age
+----+-----+
| age|count|
+----+-----+
|  19|    1|
|null|    1|
| 100|    2|
|  30|    2|
+----+-----+

Printing everything from table using sql
+----+---------+
| age|     name|
+----+---------+
|null|  Michael|
|  30|     Andy|
|  19|   Justin|
| 100|   Albert|
| 100|     Niel|
|  30|Vargheese|
+----+---------+

selectting columns based on a filter using sql
+---+---------+
|age|     name|
+---+---------+
| 30|     Andy|
| 30|Vargheese|
+---+---------+

#+end_example



* changing the column name of dataframe
http://stackoverflow.com/questions/33778664/spark-dataframe-distinguish-columns-with-duplicated-name
Column rename example 

f0 = flatClustering0.withColumnRenamed('item', 'item_jan')
f1 = flatClustering1.withColumnRenamed('item', 'item_may')

Like Relational algebra rename operation

* GroupBy
#+BEGIN_SRC python :results output
# Warning: do not import * here; only import specifically needed names
from pyspark.sql.functions import explode
from pyspark.sql.types import StructField, StructType, StringType, ArrayType
from pyspark.sql.window import Window
import pyspark.sql.functions as F
import re
from pyspark import SparkContext, SparkConf
#conf = SparkConf().setAppName("dummy app name").setMaster("local")
sc = SparkContext("local", "my app name")

from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

lst1 = [(i, 2 * i, 3 * i, i ** 2) for i in range(1, 5)]
rdd = sc.parallelize(lst1)
print rdd.collect()
ri = rdd.groupBy(lambda x: x[0])

print ri.collect()
print ri.mapValues(list).take(2)
#+END_SRC

#+RESULTS:
: [(1, 2, 3, 1), (2, 4, 6, 4), (3, 6, 9, 9), (4, 8, 12, 16)]
: [(1, <pyspark.resultiterable.ResultIterable object at 0x7f18af3c6a10>), (2, <pyspark.resultiterable.ResultIterable object at 0x7f18af3f9dd0>), (3, <pyspark.resultiterable.ResultIterable object at 0x7f18ae92c350>), (4, <pyspark.resultiterable.ResultIterable object at 0x7f18ae92c3d0>)]
: [(1, [(1, 2, 3, 1)]), (2, [(2, 4, 6, 4)])]


* Boilerplate code in all spark programs
#+BEGIN_SRC python results output
from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")

from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

#+END_SRC
* How to take a sample of a large rdd?

 |                                                                                            |
 | sample(self, withReplacement, fraction, seed=None)                                         |
 | Return a sampled subset of this RDD.                                                       |
 |                                                                                            |
 | :param withReplacement: can elements be sampled multiple times (replaced when sampled out) |
 | :param fraction: expected size of the sample as a fraction of this RDD's size              |
 | without replacement: probability that each element is chosen; fraction must be [0, 1]      |
 | with replacement: expected number of times each element is chosen; fraction must be >= 0   |
 | :param seed: seed for the random number generator                                          |
 |                                                                                            |
 | >>> rdd = sc.parallelize(range(100), 4)                                                    |
 | >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14                                          |
 | True                                                                                       |
 |                                                                                            |
 | sampleByKey(self, withReplacement, fractions, seed=None)                                   |
 | Return a subset of this RDD sampled by key (via stratified sampling).                      |
 | Create a sample of this RDD using variable sampling rates for                              |
 | different keys as specified by fractions, a key to sampling rate map.                      |
 |                                                                                            |
 | >>> fractions = {"a": 0.2, "b": 0.1}                                                       |
 | >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))       |
 | >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())             |
 | >>> 100 < len(sample["a"]) < 300 and 50 < len(sample["b"]) < 150                           |
 | True                                                                                       |
 | >>> max(sample["a"]) <= 999 and min(sample["a"]) >= 0                                      |
 | True                                                                                       |
 | >>> max(sample["b"]) <= 999 and min(sample["b"]) >= 0                                      |
 | True                                                                                       |
 |                                                                                            |
#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")

from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

rdd1 = sc.parallelize(range(100))
#print help(rdd1)
rdd2 = rdd1.sample(False, 0.1)
print rdd2.collect()

l = [('Alice', 1)]
df1 = sqlContext.createDataFrame(l)
print df1.collect()
print help(df1)
#+END_SRC

* How to take a sample of a large dataframe?
|  sample(self, withReplacement, fraction, seed=None)
 |      Returns a sampled subset of this :class:`DataFrame`.
 |      
 |      >>> df.sample(False, 0.5, 42).count()
 |      2

#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf
from pyspark.sql import Row
sc = SparkContext("local", "my app name")

from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

lst1 = [Row(name='Name%s' %i, age=i, height=i)  for i in range(1, 101)]
df1 = sc.parallelize(lst1).toDF()
df2 = df1.sample(False, .05)
df2.show()
#+END_SRC

#+RESULTS:
#+begin_example
+---+------+------+
|age|height|  name|
+---+------+------+
|  4|     4| Name4|
| 36|    36|Name36|
| 38|    38|Name38|
| 56|    56|Name56|
| 58|    58|Name58|
| 65|    65|Name65|
+---+------+------+

#+end_example

* How to save rdd as text file?
 | saveAsTextFile(self, path, compressionCodecClass=None)                                   |
 | Save this RDD as a text file, using string representations of elements.                  |
 |                                                                                          |
 | @param path: path to text file                                                           |
 | @param compressionCodecClass: (None by default) string i.e.                              |
 | "org.apache.hadoop.io.compress.GzipCodec"                                                |
 |                                                                                          |
 | >>> tempFile = NamedTemporaryFile(delete=True)                                           |
 | >>> tempFile.close()                                                                     |
 | >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)                              |
 | >>> from fileinput import input                                                          |
 | >>> from glob import glob                                                                |
 | >>> ''.join(sorted(input(glob(tempFile.name + "/part-0000*"))))                          |
 | '0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n'                                                         |
 |                                                                                          |
 | Empty lines are tolerated when saving to text files.                                     |
 |                                                                                          |
 | >>> tempFile2 = NamedTemporaryFile(delete=True)                                          |
 | >>> tempFile2.close()                                                                    |
 | >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)            |
 | >>> ''.join(sorted(input(glob(tempFile2.name + "/part-0000*"))))                         |
 | '\n\n\nbar\nfoo\n'                                                                       |
 |                                                                                          |
 | Using compressionCodecClass                                                              |
 |                                                                                          |
 | >>> tempFile3 = NamedTemporaryFile(delete=True)                                          |
 | >>> tempFile3.close()                                                                    |
 | >>> codec = "org.apache.hadoop.io.compress.GzipCodec"                                    |
 | >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)                 |
 | >>> from fileinput import input, hook_compressed                                         |
 | >>> result = sorted(input(glob(tempFile3.name + "/part*.gz"), openhook=hook_compressed)) |
 | >>> b''.join(result).decode('utf-8')                                                     |
 | u'bar\nfoo\n'                                                                            |

#+BEGIN_SRC python :results output :tangle yes :tangle /tmp/rdd_save_as_textfile.py

from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext
import tempfile
import os
sc = SparkContext("local", "my app name")
sqlContext = SQLContext(sc)

rdd1 = sc.parallelize(range(10))

filename = '/tmp/saved'
os.remove(filename)
rdd1.saveAsTextFile(filename)

#+END_SRC

#+RESULTS:

* How to save rdd to amazon s3?
#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")

from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

import ConfigParser
import os

class S3ConfParser(ConfigParser.ConfigParser):

    def as_dict(self):
        d = dict(self._sections)
        for k in d:
            d[k] = dict(self._defaults, **d[k])
            d[k].pop('__name__', None)
        return d
        
filename = '/home/bineesh/temp/only_stored_locally/s3cfg.liang'
cp = S3ConfParser()
cp.read(filename)
AccessKey = cp.get('default', 'access_key')
SecretKey = cp.get('default', 'secret_key')
AwsBucketName = None
bucket = 'dais-ng'
directory = 'work_bineesh'
full_path = ['block_out_unsorted', 'part.txt']


path = os.path.join(directory, *full_path)


def fileAtS3Path(p, bucket=AwsBucketName):
  return 's3n://{}:{}@{}/{}'.format(AccessKey, SecretKey, bucket, p)

print fileAtS3Path(path, bucket)

path1 = os.path.join(directory, 'to_save_rdd.txt')
print path1
newpath = fileAtS3Path(path1, bucket)
print newpath

rdd1 = sc.parallelize(range(10))
rdd1.saveAsTextFile(newpath)
'''
rdd1.saveAsTextFile(s"s3n://$AccessKey:$SecretKey@$AwsBucketName/temp.txt")
'''
#+END_SRC

#+RESULTS:

* How to create a dataframe from row?

#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf
from pyspark.sql import Row
sc = SparkContext("local", "my app name")

from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
a = [Row(record_index=376549, ot=[u'vasin_S-7406-2016', u'Vasin', u'V', u'Victor', None, None, u'S-7406-2016'], cluster_id=u'vasin_S-7406-2016', profile_ut=u'WOS:000070627200023', name=u'Vasin, VA', pos=u'3', similarity=0.8161098161098161, rank=1)]

xrdd = sc.parallelize(a)
print xrdd, type(xrdd)
xdf = xrdd.toDF()
print xdf, type(xdf)
#+END_SRC

#+RESULTS:
: ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:475 <class 'pyspark.rdd.RDD'>
: DataFrame[cluster_id: string, name: string, ot: array<string>, pos: string, profile_ut: string, rank: bigint, record_index: bigint, similarity: double] <class 'pyspark.sql.dataframe.DataFrame'>
* Help docstring of dataframes
Help on DataFrame in module pyspark.sql.dataframe object:

class DataFrame(__builtin__.object)
 |  A distributed collection of data grouped into named columns.
 |  
 |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,
 |  and can be created using various functions in :class:`SQLContext`::
 |  
 |      people = sqlContext.read.parquet("...")
 |  
 |  Once created, it can be manipulated using the various domain-specific-language
 |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.
 |  
 |  To select a column from the data frame, use the apply method::
 |  
 |      ageCol = people.age
 |  
 |  A more concrete example::
 |  
 |      # To create DataFrame using SQLContext
 |      people = sqlContext.read.parquet("...")
 |      department = sqlContext.read.parquet("...")
 |  
 |      people.filter(people.age > 30).join(department, people.deptId == department.id)          .groupBy(department.name, "gender").agg({"salary": "avg", "age": "max"})
 |  
 |  .. versionadded:: 1.3
 |  
 |  Methods defined here:
 |  
 |  __getattr__(self, name)
 |      Returns the :class:`Column` denoted by ``name``.
 |      
 |      >>> df.select(df.age).collect()
 |      [Row(age=2), Row(age=5)]
 |      
 |      .. versionadded:: 1.3
 |  
 |  __getitem__(self, item)
 |      Returns the column as a :class:`Column`.
 |      
 |      >>> df.select(df['age']).collect()
 |      [Row(age=2), Row(age=5)]
 |      >>> df[ ["name", "age"]].collect()
 |      [Row(name=u'Alice', age=2), Row(name=u'Bob', age=5)]
 |      >>> df[ df.age > 3 ].collect()
 |      [Row(age=5, name=u'Bob')]
 |      >>> df[df[0] > 3].collect()
 |      [Row(age=5, name=u'Bob')]
 |      
 |      .. versionadded:: 1.3
 |  
 |  __init__(self, jdf, sql_ctx)
 |  
 |  __repr__(self)
 |  
 |  agg(self, *exprs)
 |      Aggregate on the entire :class:`DataFrame` without groups
 |      (shorthand for ``df.groupBy.agg()``).
 |      
 |      >>> df.agg({"age": "max"}).collect()
 |      [Row(max(age)=5)]
 |      >>> from pyspark.sql import functions as F
 |      >>> df.agg(F.min(df.age)).collect()
 |      [Row(min(age)=2)]
 |      
 |      .. versionadded:: 1.3
 |  
 |  alias(self, alias)
 |      Returns a new :class:`DataFrame` with an alias set.
 |      
 |      >>> from pyspark.sql.functions import *
 |      >>> df_as1 = df.alias("df_as1")
 |      >>> df_as2 = df.alias("df_as2")
 |      >>> joined_df = df_as1.join(df_as2, col("df_as1.name") == col("df_as2.name"), 'inner')
 |      >>> joined_df.select("df_as1.name", "df_as2.name", "df_as2.age").collect()
 |      [Row(name=u'Bob', name=u'Bob', age=5), Row(name=u'Alice', name=u'Alice', age=2)]
 |      
 |      .. versionadded:: 1.3
 |  
 |  approxQuantile(self, col, probabilities, relativeError)
 |      Calculates the approximate quantiles of a numerical column of a
 |      DataFrame.
 |      
 |      The result of this algorithm has the following deterministic bound:
 |      If the DataFrame has N elements and if we request the quantile at
 |      probability `p` up to error `err`, then the algorithm will return
 |      a sample `x` from the DataFrame so that the *exact* rank of `x` is
 |      close to (p * N). More precisely,
 |      
 |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).
 |      
 |      This method implements a variation of the Greenwald-Khanna
 |      algorithm (with some speed optimizations). The algorithm was first
 |      present in [[http://dx.doi.org/10.1145/375663.375670
 |      Space-efficient Online Computation of Quantile Summaries]]
 |      by Greenwald and Khanna.
 |      
 |      :param col: the name of the numerical column
 |      :param probabilities: a list of quantile probabilities
 |        Each number must belong to [0, 1].
 |        For example 0 is the minimum, 0.5 is the median, 1 is the maximum.
 |      :param relativeError:  The relative target precision to achieve
 |        (>= 0). If set to zero, the exact quantiles are computed, which
 |        could be very expensive. Note that values greater than 1 are
 |        accepted but give the same result as 1.
 |      :return:  the approximate quantiles at the given probabilities
 |      
 |      .. versionadded:: 2.0
 |  
 |  cache(self)
 |      Persists with the default storage level (C{MEMORY_ONLY}).
 |      
 |      .. versionadded:: 1.3
 |  
 |  coalesce(self, numPartitions)
 |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.
 |      
 |      Similar to coalesce defined on an :class:`RDD`, this operation results in a
 |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,
 |      there will not be a shuffle, instead each of the 100 new partitions will
 |      claim 10 of the current partitions.
 |      
 |      >>> df.coalesce(1).rdd.getNumPartitions()
 |      1
 |      
 |      .. versionadded:: 1.4
 |  
 |  collect(self)
 |      Returns all the records as a list of :class:`Row`.
 |      
 |      >>> df.collect()
 |      [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]
 |      
 |      .. versionadded:: 1.3
 |  
 |  corr(self, col1, col2, method=None)
 |      Calculates the correlation of two columns of a DataFrame as a double value.
 |      Currently only supports the Pearson Correlation Coefficient.
 |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.
 |      
 |      :param col1: The name of the first column
 |      :param col2: The name of the second column
 |      :param method: The correlation method. Currently only supports "pearson"
 |      
 |      .. versionadded:: 1.4
 |  
 |  count(self)
 |      Returns the number of rows in this :class:`DataFrame`.
 |      
 |      >>> df.count()
 |      2
 |      
 |      .. versionadded:: 1.3
 |  
 |  cov(self, col1, col2)
 |      Calculate the sample covariance for the given columns, specified by their names, as a
 |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.
 |      
 |      :param col1: The name of the first column
 |      :param col2: The name of the second column
 |      
 |      .. versionadded:: 1.4
 |  
 |  createOrReplaceTempView(self, name)
 |      Creates or replaces a temporary view with this DataFrame.
 |      
 |      The lifetime of this temporary table is tied to the :class:`SparkSession`
 |      that was used to create this :class:`DataFrame`.
 |      
 |      >>> df.createOrReplaceTempView("people")
 |      >>> df2 = df.filter(df.age > 3)
 |      >>> df2.createOrReplaceTempView("people")
 |      >>> df3 = spark.sql("select * from people")
 |      >>> sorted(df3.collect()) == sorted(df2.collect())
 |      True
 |      >>> spark.catalog.dropTempView("people")
 |      
 |      .. versionadded:: 2.0
 |  
 |  createTempView(self, name)
 |      Creates a temporary view with this DataFrame.
 |      
 |      The lifetime of this temporary table is tied to the :class:`SparkSession`
 |      that was used to create this :class:`DataFrame`.
 |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the
 |      catalog.
 |      
 |      >>> df.createTempView("people")
 |      >>> df2 = spark.sql("select * from people")
 |      >>> sorted(df.collect()) == sorted(df2.collect())
 |      True
 |      >>> df.createTempView("people")  # doctest: +IGNORE_EXCEPTION_DETAIL
 |      Traceback (most recent call last):
 |      ...
 |      AnalysisException: u"Temporary table 'people' already exists;"
 |      >>> spark.catalog.dropTempView("people")
 |      
 |      .. versionadded:: 2.0
 |  
 |  crosstab(self, col1, col2)
 |      Computes a pair-wise frequency table of the given columns. Also known as a contingency
 |      table. The number of distinct values for each column should be less than 1e4. At most 1e6
 |      non-zero pair frequencies will be returned.
 |      The first column of each row will be the distinct values of `col1` and the column names
 |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.
 |      Pairs that have no occurrences will have zero as their counts.
 |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.
 |      
 |      :param col1: The name of the first column. Distinct items will make the first item of
 |          each row.
 |      :param col2: The name of the second column. Distinct items will make the column names
 |          of the DataFrame.
 |      
 |      .. versionadded:: 1.4
 |  
 |  cube(self, *cols)
 |      Create a multi-dimensional cube for the current :class:`DataFrame` using
 |      the specified columns, so we can run aggregation on them.
 |      
 |      >>> df.cube("name", df.age).count().orderBy("name", "age").show()
 |      +-----+----+-----+
 |      | name| age|count|
 |      +-----+----+-----+
 |      | null|null|    2|
 |      | null|   2|    1|
 |      | null|   5|    1|
 |      |Alice|null|    1|
 |      |Alice|   2|    1|
 |      |  Bob|null|    1|
 |      |  Bob|   5|    1|
 |      +-----+----+-----+
 |      
 |      .. versionadded:: 1.4
 |  
 |  describe(self, *cols)
 |      Computes statistics for numeric columns.
 |      
 |      This include count, mean, stddev, min, and max. If no columns are
 |      given, this function computes statistics for all numerical columns.
 |      
 |      .. note:: This function is meant for exploratory data analysis, as we make no         guarantee about the backward compatibility of the schema of the resulting DataFrame.
 |      
 |      >>> df.describe().show()
 |      +-------+------------------+
 |      |summary|               age|
 |      +-------+------------------+
 |      |  count|                 2|
 |      |   mean|               3.5|
 |      | stddev|2.1213203435596424|
 |      |    min|                 2|
 |      |    max|                 5|
 |      +-------+------------------+
 |      >>> df.describe(['age', 'name']).show()
 |      +-------+------------------+-----+
 |      |summary|               age| name|
 |      +-------+------------------+-----+
 |      |  count|                 2|    2|
 |      |   mean|               3.5| null|
 |      | stddev|2.1213203435596424| null|
 |      |    min|                 2|Alice|
 |      |    max|                 5|  Bob|
 |      +-------+------------------+-----+
 |      
 |      .. versionadded:: 1.3.1
 |  
 |  distinct(self)
 |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.
 |      
 |      >>> df.distinct().count()
 |      2
 |      
 |      .. versionadded:: 1.3
 |  
 |  drop(self, col)
 |      Returns a new :class:`DataFrame` that drops the specified column.
 |      
 |      :param col: a string name of the column to drop, or a
 |          :class:`Column` to drop.
 |      
 |      >>> df.drop('age').collect()
 |      [Row(name=u'Alice'), Row(name=u'Bob')]
 |      
 |      >>> df.drop(df.age).collect()
 |      [Row(name=u'Alice'), Row(name=u'Bob')]
 |      
 |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()
 |      [Row(age=5, height=85, name=u'Bob')]
 |      
 |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()
 |      [Row(age=5, name=u'Bob', height=85)]
 |      
 |      .. versionadded:: 1.4
 |  
 |  dropDuplicates(self, subset=None)
 |      Return a new :class:`DataFrame` with duplicate rows removed,
 |      optionally only considering certain columns.
 |      
 |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.
 |      
 |      >>> from pyspark.sql import Row
 |      >>> df = sc.parallelize([ \
 |      ...     Row(name='Alice', age=5, height=80), \
 |      ...     Row(name='Alice', age=5, height=80), \
 |      ...     Row(name='Alice', age=10, height=80)]).toDF()
 |      >>> df.dropDuplicates().show()
 |      +---+------+-----+
 |      |age|height| name|
 |      +---+------+-----+
 |      |  5|    80|Alice|
 |      | 10|    80|Alice|
 |      +---+------+-----+
 |      
 |      >>> df.dropDuplicates(['name', 'height']).show()
 |      +---+------+-----+
 |      |age|height| name|
 |      +---+------+-----+
 |      |  5|    80|Alice|
 |      +---+------+-----+
 |      
 |      .. versionadded:: 1.4
 |  
 |  drop_duplicates = dropDuplicates(self, subset=None)
 |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.
 |      
 |      .. versionadded:: 1.4
 |  
 |  dropna(self, how='any', thresh=None, subset=None)
 |      Returns a new :class:`DataFrame` omitting rows with null values.
 |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.
 |      
 |      :param how: 'any' or 'all'.
 |          If 'any', drop a row if it contains any nulls.
 |          If 'all', drop a row only if all its values are null.
 |      :param thresh: int, default None
 |          If specified, drop rows that have less than `thresh` non-null values.
 |          This overwrites the `how` parameter.
 |      :param subset: optional list of column names to consider.
 |      
 |      >>> df4.na.drop().show()
 |      +---+------+-----+
 |      |age|height| name|
 |      +---+------+-----+
 |      | 10|    80|Alice|
 |      +---+------+-----+
 |      
 |      .. versionadded:: 1.3.1
 |  
 |  explain(self, extended=False)
 |      Prints the (logical and physical) plans to the console for debugging purpose.
 |      
 |      :param extended: boolean, default ``False``. If ``False``, prints only the physical plan.
 |      
 |      >>> df.explain()
 |      == Physical Plan ==
 |      Scan ExistingRDD[age#0,name#1]
 |      
 |      >>> df.explain(True)
 |      == Parsed Logical Plan ==
 |      ...
 |      == Analyzed Logical Plan ==
 |      ...
 |      == Optimized Logical Plan ==
 |      ...
 |      == Physical Plan ==
 |      ...
 |      
 |      .. versionadded:: 1.3
 |  
 |  fillna(self, value, subset=None)
 |      Replace null values, alias for ``na.fill()``.
 |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.
 |      
 |      :param value: int, long, float, string, or dict.
 |          Value to replace null values with.
 |          If the value is a dict, then `subset` is ignored and `value` must be a mapping
 |          from column name (string) to replacement value. The replacement value must be
 |          an int, long, float, or string.
 |      :param subset: optional list of column names to consider.
 |          Columns specified in subset that do not have matching data type are ignored.
 |          For example, if `value` is a string, and subset contains a non-string column,
 |          then the non-string column is simply ignored.
 |      
 |      >>> df4.na.fill(50).show()
 |      +---+------+-----+
 |      |age|height| name|
 |      +---+------+-----+
 |      | 10|    80|Alice|
 |      |  5|    50|  Bob|
 |      | 50|    50|  Tom|
 |      | 50|    50| null|
 |      +---+------+-----+
 |      
 |      >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()
 |      +---+------+-------+
 |      |age|height|   name|
 |      +---+------+-------+
 |      | 10|    80|  Alice|
 |      |  5|  null|    Bob|
 |      | 50|  null|    Tom|
 |      | 50|  null|unknown|
 |      +---+------+-------+
 |      
 |      .. versionadded:: 1.3.1
 |  
 |  filter(self, condition)
 |      Filters rows using the given condition.
 |      
 |      :func:`where` is an alias for :func:`filter`.
 |      
 |      :param condition: a :class:`Column` of :class:`types.BooleanType`
 |          or a string of SQL expression.
 |      
 |      >>> df.filter(df.age > 3).collect()
 |      [Row(age=5, name=u'Bob')]
 |      >>> df.where(df.age == 2).collect()
 |      [Row(age=2, name=u'Alice')]
 |      
 |      >>> df.filter("age > 3").collect()
 |      [Row(age=5, name=u'Bob')]
 |      >>> df.where("age = 2").collect()
 |      [Row(age=2, name=u'Alice')]
 |      
 |      .. versionadded:: 1.3
 |  
 |  first(self)
 |      Returns the first row as a :class:`Row`.
 |      
 |      >>> df.first()
 |      Row(age=2, name=u'Alice')
 |      
 |      .. versionadded:: 1.3
 |  
 |  foreach(self, f)
 |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.
 |      
 |      This is a shorthand for ``df.rdd.foreach()``.
 |      
 |      >>> def f(person):
 |      ...     print(person.name)
 |      >>> df.foreach(f)
 |      
 |      .. versionadded:: 1.3
 |  
 |  foreachPartition(self, f)
 |      Applies the ``f`` function to each partition of this :class:`DataFrame`.
 |      
 |      This a shorthand for ``df.rdd.foreachPartition()``.
 |      
 |      >>> def f(people):
 |      ...     for person in people:
 |      ...         print(person.name)
 |      >>> df.foreachPartition(f)
 |      
 |      .. versionadded:: 1.3
 |  
 |  freqItems(self, cols, support=None)
 |      Finding frequent items for columns, possibly with false positives. Using the
 |      frequent element count algorithm described in
 |      "http://dx.doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou".
 |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.
 |      
 |      .. note::  This function is meant for exploratory data analysis, as we make no         guarantee about the backward compatibility of the schema of the resulting DataFrame.
 |      
 |      :param cols: Names of the columns to calculate frequent items for as a list or tuple of
 |          strings.
 |      :param support: The frequency with which to consider an item 'frequent'. Default is 1%.
 |          The support must be greater than 1e-4.
 |      
 |      .. versionadded:: 1.4
 |  
 |  groupBy(self, *cols)
 |      Groups the :class:`DataFrame` using the specified columns,
 |      so we can run aggregation on them. See :class:`GroupedData`
 |      for all the available aggregate functions.
 |      
 |      :func:`groupby` is an alias for :func:`groupBy`.
 |      
 |      :param cols: list of columns to group by.
 |          Each element should be a column name (string) or an expression (:class:`Column`).
 |      
 |      >>> df.groupBy().avg().collect()
 |      [Row(avg(age)=3.5)]
 |      >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())
 |      [Row(name=u'Alice', avg(age)=2.0), Row(name=u'Bob', avg(age)=5.0)]
 |      >>> sorted(df.groupBy(df.name).avg().collect())
 |      [Row(name=u'Alice', avg(age)=2.0), Row(name=u'Bob', avg(age)=5.0)]
 |      >>> sorted(df.groupBy(['name', df.age]).count().collect())
 |      [Row(name=u'Alice', age=2, count=1), Row(name=u'Bob', age=5, count=1)]
 |      
 |      .. versionadded:: 1.3
 |  
 |  groupby = groupBy(self, *cols)
 |      :func:`groupby` is an alias for :func:`groupBy`.
 |      
 |      .. versionadded:: 1.4
 |  
 |  head(self, n=None)
 |      Returns the first ``n`` rows.
 |      
 |      Note that this method should only be used if the resulting array is expected
 |      to be small, as all the data is loaded into the driver's memory.
 |      
 |      :param n: int, default 1. Number of rows to return.
 |      :return: If n is greater than 1, return a list of :class:`Row`.
 |          If n is 1, return a single Row.
 |      
 |      >>> df.head()
 |      Row(age=2, name=u'Alice')
 |      >>> df.head(1)
 |      [Row(age=2, name=u'Alice')]
 |      
 |      .. versionadded:: 1.3
 |  
 |  intersect(self, other)
 |      Return a new :class:`DataFrame` containing rows only in
 |      both this frame and another frame.
 |      
 |      This is equivalent to `INTERSECT` in SQL.
 |      
 |      .. versionadded:: 1.3
 |  
 |  isLocal(self)
 |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally
 |      (without any Spark executors).
 |      
 |      .. versionadded:: 1.3
 |  
 |  join(self, other, on=None, how=None)
 |      Joins with another :class:`DataFrame`, using the given join expression.
 |      
 |      :param other: Right side of the join
 |      :param on: a string for the join column name, a list of column names,
 |          a join expression (Column), or a list of Columns.
 |          If `on` is a string or a list of strings indicating the name of the join column(s),
 |          the column(s) must exist on both sides, and this performs an equi-join.
 |      :param how: str, default 'inner'.
 |          One of `inner`, `outer`, `left_outer`, `right_outer`, `leftsemi`.
 |      
 |      The following performs a full outer join between ``df1`` and ``df2``.
 |      
 |      >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height).collect()
 |      [Row(name=None, height=80), Row(name=u'Bob', height=85), Row(name=u'Alice', height=None)]
 |      
 |      >>> df.join(df2, 'name', 'outer').select('name', 'height').collect()
 |      [Row(name=u'Tom', height=80), Row(name=u'Bob', height=85), Row(name=u'Alice', height=None)]
 |      
 |      >>> cond = [df.name == df3.name, df.age == df3.age]
 |      >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()
 |      [Row(name=u'Alice', age=2), Row(name=u'Bob', age=5)]
 |      
 |      >>> df.join(df2, 'name').select(df.name, df2.height).collect()
 |      [Row(name=u'Bob', height=85)]
 |      
 |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()
 |      [Row(name=u'Bob', age=5)]
 |      
 |      .. versionadded:: 1.3
 |  
 |  limit(self, num)
 |      Limits the result count to the number specified.
 |      
 |      >>> df.limit(1).collect()
 |      [Row(age=2, name=u'Alice')]
 |      >>> df.limit(0).collect()
 |      []
 |      
 |      .. versionadded:: 1.3
 |  
 |  orderBy = sort(self, *cols, **kwargs)
 |  
 |  persist(self, storageLevel=StorageLevel(False, True, False, False, 1))
 |      Sets the storage level to persist its values across operations
 |      after the first time it is computed. This can only be used to assign
 |      a new storage level if the RDD does not have a storage level set yet.
 |      If no storage level is specified defaults to (C{MEMORY_ONLY}).
 |      
 |      .. versionadded:: 1.3
 |  
 |  printSchema(self)
 |      Prints out the schema in the tree format.
 |      
 |      >>> df.printSchema()
 |      root
 |       |-- age: integer (nullable = true)
 |       |-- name: string (nullable = true)
 |      <BLANKLINE>
 |      
 |      .. versionadded:: 1.3
 |  
 |  randomSplit(self, weights, seed=None)
 |      Randomly splits this :class:`DataFrame` with the provided weights.
 |      
 |      :param weights: list of doubles as weights with which to split the DataFrame. Weights will
 |          be normalized if they don't sum up to 1.0.
 |      :param seed: The seed for sampling.
 |      
 |      >>> splits = df4.randomSplit([1.0, 2.0], 24)
 |      >>> splits[0].count()
 |      1
 |      
 |      >>> splits[1].count()
 |      3
 |      
 |      .. versionadded:: 1.4
 |  
 |  registerTempTable(self, name)
 |      Registers this RDD as a temporary table using the given name.
 |      
 |      The lifetime of this temporary table is tied to the :class:`SQLContext`
 |      that was used to create this :class:`DataFrame`.
 |      
 |      >>> df.registerTempTable("people")
 |      >>> df2 = spark.sql("select * from people")
 |      >>> sorted(df.collect()) == sorted(df2.collect())
 |      True
 |      >>> spark.catalog.dropTempView("people")
 |      
 |      .. note:: Deprecated in 2.0, use createOrReplaceTempView instead.
 |      
 |      .. versionadded:: 1.3
 |  
 |  repartition(self, numPartitions, *cols)
 |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The
 |      resulting DataFrame is hash partitioned.
 |      
 |      ``numPartitions`` can be an int to specify the target number of partitions or a Column.
 |      If it is a Column, it will be used as the first partitioning column. If not specified,
 |      the default number of partitions is used.
 |      
 |      .. versionchanged:: 1.6
 |         Added optional arguments to specify the partitioning columns. Also made numPartitions
 |         optional if partitioning columns are specified.
 |      
 |      >>> df.repartition(10).rdd.getNumPartitions()
 |      10
 |      >>> data = df.union(df).repartition("age")
 |      >>> data.show()
 |      +---+-----+
 |      |age| name|
 |      +---+-----+
 |      |  5|  Bob|
 |      |  5|  Bob|
 |      |  2|Alice|
 |      |  2|Alice|
 |      +---+-----+
 |      >>> data = data.repartition(7, "age")
 |      >>> data.show()
 |      +---+-----+
 |      |age| name|
 |      +---+-----+
 |      |  5|  Bob|
 |      |  5|  Bob|
 |      |  2|Alice|
 |      |  2|Alice|
 |      +---+-----+
 |      >>> data.rdd.getNumPartitions()
 |      7
 |      >>> data = data.repartition("name", "age")
 |      >>> data.show()
 |      +---+-----+
 |      |age| name|
 |      +---+-----+
 |      |  5|  Bob|
 |      |  5|  Bob|
 |      |  2|Alice|
 |      |  2|Alice|
 |      +---+-----+
 |      
 |      .. versionadded:: 1.3
 |  
 |  replace(self, to_replace, value, subset=None)
 |      Returns a new :class:`DataFrame` replacing a value with another value.
 |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are
 |      aliases of each other.
 |      
 |      :param to_replace: int, long, float, string, or list.
 |          Value to be replaced.
 |          If the value is a dict, then `value` is ignored and `to_replace` must be a
 |          mapping from column name (string) to replacement value. The value to be
 |          replaced must be an int, long, float, or string.
 |      :param value: int, long, float, string, or list.
 |          Value to use to replace holes.
 |          The replacement value must be an int, long, float, or string. If `value` is a
 |          list or tuple, `value` should be of the same length with `to_replace`.
 |      :param subset: optional list of column names to consider.
 |          Columns specified in subset that do not have matching data type are ignored.
 |          For example, if `value` is a string, and subset contains a non-string column,
 |          then the non-string column is simply ignored.
 |      
 |      >>> df4.na.replace(10, 20).show()
 |      +----+------+-----+
 |      | age|height| name|
 |      +----+------+-----+
 |      |  20|    80|Alice|
 |      |   5|  null|  Bob|
 |      |null|  null|  Tom|
 |      |null|  null| null|
 |      +----+------+-----+
 |      
 |      >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()
 |      +----+------+----+
 |      | age|height|name|
 |      +----+------+----+
 |      |  10|    80|   A|
 |      |   5|  null|   B|
 |      |null|  null| Tom|
 |      |null|  null|null|
 |      +----+------+----+
 |      
 |      .. versionadded:: 1.4
 |  
 |  rollup(self, *cols)
 |      Create a multi-dimensional rollup for the current :class:`DataFrame` using
 |      the specified columns, so we can run aggregation on them.
 |      
 |      >>> df.rollup("name", df.age).count().orderBy("name", "age").show()
 |      +-----+----+-----+
 |      | name| age|count|
 |      +-----+----+-----+
 |      | null|null|    2|
 |      |Alice|null|    1|
 |      |Alice|   2|    1|
 |      |  Bob|null|    1|
 |      |  Bob|   5|    1|
 |      +-----+----+-----+
 |      
 |      .. versionadded:: 1.4
 |  
 |  sample(self, withReplacement, fraction, seed=None)
 |      Returns a sampled subset of this :class:`DataFrame`.
 |      
 |      >>> df.sample(False, 0.5, 42).count()
 |      2
 |      
 |      .. versionadded:: 1.3
 |  
 |  sampleBy(self, col, fractions, seed=None)
 |      Returns a stratified sample without replacement based on the
 |      fraction given on each stratum.
 |      
 |      :param col: column that defines strata
 |      :param fractions:
 |          sampling fraction for each stratum. If a stratum is not
 |          specified, we treat its fraction as zero.
 |      :param seed: random seed
 |      :return: a new DataFrame that represents the stratified sample
 |      
 |      >>> from pyspark.sql.functions import col
 |      >>> dataset = sqlContext.range(0, 100).select((col("id") % 3).alias("key"))
 |      >>> sampled = dataset.sampleBy("key", fractions={0: 0.1, 1: 0.2}, seed=0)
 |      >>> sampled.groupBy("key").count().orderBy("key").show()
 |      +---+-----+
 |      |key|count|
 |      +---+-----+
 |      |  0|    5|
 |      |  1|    9|
 |      +---+-----+
 |      
 |      .. versionadded:: 1.5
 |  
 |  select(self, *cols)
 |      Projects a set of expressions and returns a new :class:`DataFrame`.
 |      
 |      :param cols: list of column names (string) or expressions (:class:`Column`).
 |          If one of the column names is '*', that column is expanded to include all columns
 |          in the current DataFrame.
 |      
 |      >>> df.select('*').collect()
 |      [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]
 |      >>> df.select('name', 'age').collect()
 |      [Row(name=u'Alice', age=2), Row(name=u'Bob', age=5)]
 |      >>> df.select(df.name, (df.age + 10).alias('age')).collect()
 |      [Row(name=u'Alice', age=12), Row(name=u'Bob', age=15)]
 |      
 |      .. versionadded:: 1.3
 |  
 |  selectExpr(self, *expr)
 |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.
 |      
 |      This is a variant of :func:`select` that accepts SQL expressions.
 |      
 |      >>> df.selectExpr("age * 2", "abs(age)").collect()
 |      [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]
 |      
 |      .. versionadded:: 1.3
 |  
 |  show(self, n=20, truncate=True)
 |      Prints the first ``n`` rows to the console.
 |      
 |      :param n: Number of rows to show.
 |      :param truncate: Whether truncate long strings and align cells right.
 |      
 |      >>> df
 |      DataFrame[age: int, name: string]
 |      >>> df.show()
 |      +---+-----+
 |      |age| name|
 |      +---+-----+
 |      |  2|Alice|
 |      |  5|  Bob|
 |      +---+-----+
 |      
 |      .. versionadded:: 1.3
 |  
 |  sort(self, *cols, **kwargs)
 |      Returns a new :class:`DataFrame` sorted by the specified column(s).
 |      
 |      :param cols: list of :class:`Column` or column names to sort by.
 |      :param ascending: boolean or list of boolean (default True).
 |          Sort ascending vs. descending. Specify list for multiple sort orders.
 |          If a list is specified, length of the list must equal length of the `cols`.
 |      
 |      >>> df.sort(df.age.desc()).collect()
 |      [Row(age=5, name=u'Bob'), Row(age=2, name=u'Alice')]
 |      >>> df.sort("age", ascending=False).collect()
 |      [Row(age=5, name=u'Bob'), Row(age=2, name=u'Alice')]
 |      >>> df.orderBy(df.age.desc()).collect()
 |      [Row(age=5, name=u'Bob'), Row(age=2, name=u'Alice')]
 |      >>> from pyspark.sql.functions import *
 |      >>> df.sort(asc("age")).collect()
 |      [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]
 |      >>> df.orderBy(desc("age"), "name").collect()
 |      [Row(age=5, name=u'Bob'), Row(age=2, name=u'Alice')]
 |      >>> df.orderBy(["age", "name"], ascending=[0, 1]).collect()
 |      [Row(age=5, name=u'Bob'), Row(age=2, name=u'Alice')]
 |      
 |      .. versionadded:: 1.3
 |  
 |  sortWithinPartitions(self, *cols, **kwargs)
 |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).
 |      
 |      :param cols: list of :class:`Column` or column names to sort by.
 |      :param ascending: boolean or list of boolean (default True).
 |          Sort ascending vs. descending. Specify list for multiple sort orders.
 |          If a list is specified, length of the list must equal length of the `cols`.
 |      
 |      >>> df.sortWithinPartitions("age", ascending=False).show()
 |      +---+-----+
 |      |age| name|
 |      +---+-----+
 |      |  2|Alice|
 |      |  5|  Bob|
 |      +---+-----+
 |      
 |      .. versionadded:: 1.6
 |  
 |  subtract(self, other)
 |      Return a new :class:`DataFrame` containing rows in this frame
 |      but not in another frame.
 |      
 |      This is equivalent to `EXCEPT` in SQL.
 |      
 |      .. versionadded:: 1.3
 |  
 |  take(self, num)
 |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.
 |      
 |      >>> df.take(2)
 |      [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]
 |      
 |      .. versionadded:: 1.3
 |  
 |  toDF(self, *cols)
 |      Returns a new class:`DataFrame` that with new specified column names
 |      
 |      :param cols: list of new column names (string)
 |      
 |      >>> df.toDF('f1', 'f2').collect()
 |      [Row(f1=2, f2=u'Alice'), Row(f1=5, f2=u'Bob')]
 |  
 |  toJSON(self, use_unicode=True)
 |      Converts a :class:`DataFrame` into a :class:`RDD` of string.
 |      
 |      Each row is turned into a JSON document as one element in the returned RDD.
 |      
 |      >>> df.toJSON().first()
 |      u'{"age":2,"name":"Alice"}'
 |      
 |      .. versionadded:: 1.3
 |  
 |  toLocalIterator(self)
 |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.
 |      The iterator will consume as much memory as the largest partition in this DataFrame.
 |      
 |      >>> list(df.toLocalIterator())
 |      [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]
 |      
 |      .. versionadded:: 2.0
 |  
 |  toPandas(self)
 |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.
 |      
 |      Note that this method should only be used if the resulting Pandas's DataFrame is expected
 |      to be small, as all the data is loaded into the driver's memory.
 |      
 |      This is only available if Pandas is installed and available.
 |      
 |      >>> df.toPandas()  # doctest: +SKIP
 |         age   name
 |      0    2  Alice
 |      1    5    Bob
 |      
 |      .. versionadded:: 1.3
 |  
 |  union(self, other)
 |      Return a new :class:`DataFrame` containing union of rows in this
 |      frame and another frame.
 |      
 |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union
 |      (that does deduplication of elements), use this function followed by a distinct.
 |      
 |      .. versionadded:: 2.0
 |  
 |  unionAll(self, other)
 |      Return a new :class:`DataFrame` containing union of rows in this
 |      frame and another frame.
 |      
 |      .. note:: Deprecated in 2.0, use union instead.
 |      
 |      .. versionadded:: 1.3
 |  
 |  unpersist(self, blocking=False)
 |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from
 |      memory and disk.
 |      
 |      .. note:: `blocking` default has changed to False to match Scala in 2.0.
 |      
 |      .. versionadded:: 1.3
 |  
 |  where = filter(self, condition)
 |      :func:`where` is an alias for :func:`filter`.
 |      
 |      .. versionadded:: 1.3
 |  
 |  withColumn(self, colName, col)
 |      Returns a new :class:`DataFrame` by adding a column or replacing the
 |      existing column that has the same name.
 |      
 |      :param colName: string, name of the new column.
 |      :param col: a :class:`Column` expression for the new column.
 |      
 |      >>> df.withColumn('age2', df.age + 2).collect()
 |      [Row(age=2, name=u'Alice', age2=4), Row(age=5, name=u'Bob', age2=7)]
 |      
 |      .. versionadded:: 1.3
 |  
 |  withColumnRenamed(self, existing, new)
 |      Returns a new :class:`DataFrame` by renaming an existing column.
 |      
 |      :param existing: string, name of the existing column to rename.
 |      :param col: string, new name of the column.
 |      
 |      >>> df.withColumnRenamed('age', 'age2').collect()
 |      [Row(age2=2, name=u'Alice'), Row(age2=5, name=u'Bob')]
 |      
 |      .. versionadded:: 1.3
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors defined here:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)
 |  
 |  columns
 |      Returns all column names as a list.
 |      
 |      >>> df.columns
 |      ['age', 'name']
 |      
 |      .. versionadded:: 1.3
 |  
 |  dtypes
 |      Returns all column names and their data types as a list.
 |      
 |      >>> df.dtypes
 |      [('age', 'int'), ('name', 'string')]
 |      
 |      .. versionadded:: 1.3
 |  
 |  isStreaming
 |      Returns true if this :class:`Dataset` contains one or more sources that continuously
 |      return data as it arrives. A :class:`Dataset` that reads data from a streaming source
 |      must be executed as a :class:`StreamingQuery` using the :func:`start` method in
 |      :class:`DataStreamWriter`.  Methods that return a single answer, (e.g., :func:`count` or
 |      :func:`collect`) will throw an :class:`AnalysisException` when there is a streaming
 |      source present.
 |      
 |      .. note:: Experimental
 |      
 |      .. versionadded:: 2.0
 |  
 |  na
 |      Returns a :class:`DataFrameNaFunctions` for handling missing values.
 |      
 |      .. versionadded:: 1.3.1
 |  
 |  rdd
 |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.
 |      
 |      .. versionadded:: 1.3
 |  
 |  schema
 |      Returns the schema of this :class:`DataFrame` as a :class:`types.StructType`.
 |      
 |      >>> df.schema
 |      StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))
 |      
 |      .. versionadded:: 1.3
 |  
 |  stat
 |      Returns a :class:`DataFrameStatFunctions` for statistic functions.
 |      
 |      .. versionadded:: 1.4
 |  
 |  write
 |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external
 |      storage.
 |      
 |      :return: :class:`DataFrameWriter`
 |      
 |      .. versionadded:: 1.4
 |  
 |  writeStream
 |      Interface for saving the content of the streaming :class:`DataFrame` out into external
 |      storage.
 |      
 |      .. note:: Experimental.
 |      
 |      :return: :class:`DataStreamWriter`
 |      
 |      .. versionadded:: 2.0

* Read from csv idea
#+BEGIN_SRC python :results output :tangle yes :tangle /tmp/read_from_csv.py

from pyspark import SparkContext, SparkConf
try:
    sc = SparkContext("local", "my app name")
    from pyspark.sql import SQLContext
    sqlContext = SQLContext(sc)
except:
    pass

filename = '/tmp/daisng_cluster1.csv'

first_cluster = """A\t01-01|02-02|03-03|04-04
B\t05-05|06-06|07-07|08-08
C\t10-10|11-11
Z\t50-50
"""

def write_cluster(filename, filecontents):
    """
    Writes filecontents (string) to filename
    """
    with open(filename, 'w') as f:
        f.write(filecontents)


write_cluster(filename, first_cluster)

def cluster_read_helper(x):
    a, b = x.split('\t')
    c = b.split('|')
    return [[a, d] for d in c]

rdd1 = sc.textFile(filename)
rdd2 = rdd1.flatMap(lambda x: cluster_read_helper(x))
daisng_cluster_jan2017 = rdd2.toDF(['daisngid','item'])
daisng_cluster_jan2017.show()
#+END_SRC

#+RESULTS:
#+begin_example
+--------+-----+
|daisngid| item|
+--------+-----+
|       A|01-01|
|       A|02-02|
|       A|03-03|
|       A|04-04|
|       B|05-05|
|       B|06-06|
|       B|07-07|
|       B|08-08|
|       C|10-10|
|       C|11-11|
|       Z|50-50|
+--------+-----+

#+end_example

* Convert the blockout output to data frame

#+BEGIN_SRC python :results output :tangle yes :tangle /tmp/find_clusters_in_real_name.py
  from pyspark import SparkContext, SparkConf
  from pyspark.sql.types import *
  from pyspark.sql.functions import udf
  from pyspark.sql import functions as F

  try:
      sc = SparkContext("local", "my app name")
      from pyspark.sql import SQLContext
      sqlContext = SQLContext(sc)
  except:
    pass


  def get_daisng_cluster_test_data():
    cluster_filename = '/tmp/daisng_cluster1.csv'
    
    first_cluster = """A\t01-01|02-02|03-03|04-04
    B\t05-05|06-06|07-07|08-08
    C\t10-10|11-11
    D\tWOS:000250933800019-3
    Z\t50-50"""
    return cluster_filename, first_cluster


  def get_blockout_test_data():
    filename = '/tmp/block_out1.csv'

    block_out_string = """wang_y|304574466WOS1/WOS:000248320000087|2|<name dais_id="15657181" daisng_id="28404544" role="author" seq_no="2"><display_name>Wang, Y.</display_name><full_name>Wang, Y.</full_name><wos_standard>Wang, Y</wos_standard><first_name>Y.</first_name><last_name>Wang</last_name></name>||145683,26137801,145572,44449635,45283969,289327878,9607544,769652,77425829,8273236,38096655,997172,40304141,3163273,11793431,53592067,1090236,146904,5817362,2540183,3028941,5904152,57964622,6130089,321673869,39602203,771533,54275596,29200373,1001617,2623595,37904495,3963808,704559,147769,54261196,1015906,6354149,3180684,65249584,1091149,287425517,46686151,57532498,148850
    wang_y|79358886WOS1/WOS:000250933800019|3|<name dais_id="15806918" daisng_id="89384565" reprint="Y" role="author" seq_no="3"><display_name>Wang, Yuesi</display_name><full_name>Wang, Yuesi</full_name><wos_standard>Wang, YS</wos_standard><first_name>Yuesi</first_name><last_name>Wang</last_name><email_addr>wys@dq.cern.ac.cn</email_addr></name>|wys@dq.cern.ac.cn|57634036,4292101,17525689,76804443,79358939,46888289,674726,875651,59633537,372178,675737,45209525,79358996,1997459,2381233,2970196,32150724,56971715,10190930,41007706,79359078,73327323,48406428,675865,4477203,2325818,55581992,45546408,322531529,2382077,76806739,79359183,287446226,79359202,73915327,79359237,79359241,43102338,79359257"""

    return filename, block_out_string

  def get_ut(fuid_slash_ut):
      """
      Given fuid_slash_ut
      returns everything after /
      fuid/wos:ut-position
      ie: returns wos:ut-position
      """
      try:
          value = fuid_slash_ut.split('/')[1]
      except IndexError:
          value = fuid_slash_ut
      return value

  get_ut_udf = udf(get_ut, StringType())


  def write_cluster(filename, filecontents):
    """
    Writes filecontents (string) to filename
    """
    with open(filename, 'w') as f:
      f.write(filecontents)

  def cluster_read_helper(x):
      a, b = x.split('\t')
      c = b.split('|')
      return [[a, d] for d in c]


  def generate_daisng_cluster_test_data():
    cluster_filename, first_cluster = get_daisng_cluster_test_data()
    write_cluster(cluster_filename, first_cluster)
    rdd1 = sc.textFile(cluster_filename)
    rdd2 = rdd1.flatMap(lambda x: cluster_read_helper(x))
    daisng_clusters_df = rdd2.toDF(['daisngid','item'])
    daisng_clusters_df.show()
    return daisng_clusters_df



  def block_reader_helper(x):
    c = x.split('|')
    return [c]

    
  def generate_daisng_blockout_test_data():
      filename, block_out_string = get_blockout_test_data()
      write_cluster(filename, block_out_string)

      rdd1 = sc.textFile(filename)
      rdd2 = rdd1.flatMap(lambda x: block_reader_helper(x))
      block_out_df = (rdd2
                    .toDF(['blockid', 'fuid', 'pos', 'xml', 'email', 'citations']
                          )
                    )
      block_out_df.show()
      a_block_out_df = (block_out_df
                      .select('blockid',
                              get_ut_udf(F.col('fuid')),
                              'pos',
                              'xml',
                              'email')
                        .withColumnRenamed('get_ut(fuid)', 'ut')                    
                      )

      b_block_out_df = (a_block_out_df
                      .select('blockid',
                          F.concat(F.col('ut'),
                                   F.lit('-'),
                                   F.col('pos')
                                   ).alias('ut_pos'),
                              'xml',
                              'email'
                      ))
      a_block_out_df.show()
      b_block_out_df.show()

      return b_block_out_df



  def process():
      daisng_clusters_df = generate_daisng_cluster_test_data()
      b_block_out_df = generate_daisng_blockout_test_data()
    
      join_blockout_daisng = (b_block_out_df
                                .join(daisng_clusters_df,
                            on=b_block_out_df.ut_pos == daisng_clusters_df.item
                                )
      )

      #join_blockout_daisng.show()


      outdirectory = '/tmp/spark_data'
      #quick_delete(outdirectory)
      (join_blockout_daisng
       .write
       .format("com.databricks.spark.csv")
       .option("delimiter", "\t")
       .mode("overwrite")
       .save(outdirectory)
      )


  process()
#+END_SRC

#+RESULTS:


* Save as text file and save in csv file
#+BEGIN_SRC python :results output :tangle yes :tangle ~/quicksaves/save_as_text_file.py


  from pyspark import SparkContext, SparkConf
  from pyspark.sql.functions import collect_list, udf
  from pyspark.sql.types import StringType

  import os
  import shutil
  try:
      sc = SparkContext("local", "my app name")
      from pyspark.sql import SQLContext
      sqlContext = SQLContext(sc)
  except:
      pass

  filename = '/tmp/daisng_cluster1.csv'

  first_cluster = """A\t01-01|02-02|03-03|04-04
  B\t05-05|06-06|07-07|08-08
  C\t10-10|11-11
  Z\t50-50
  """

  def write_cluster(filename, filecontents):
      """
      Writes filecontents (string) to filename
      """
      with open(filename, 'w') as f:
          f.write(filecontents)


  write_cluster(filename, first_cluster)

  def cluster_read_helper(x):
      a, b = x.split('\t')
      c = b.split('|')
      return [[a, d] for d in c]

  rdd1 = sc.textFile(filename)
  rdd2 = rdd1.flatMap(lambda x: cluster_read_helper(x))
  daisng_cluster_jan2017 = rdd2.toDF(['daisngid','item'])
  daisng_cluster_jan2017.show()

  def quick_delete(filename):
      try:
          shutil.rmtree(filename)
      except OSError:
          print "Failed to delete", filename

  def quick_print_directory(directory):
      import glob
      files = glob.glob(directory + "/*")
      print "printing ", directory
      for file1 in files:
          print "...printing", file1
          with open(file1) as f:
              print f.read()

  written_directory = '/tmp/save_file_test'
  quick_delete(written_directory)
  daisng_cluster_jan2017.rdd.saveAsTextFile(written_directory)

  # os.system('cat {}/*'.format(written_directory))
  quick_print_directory(written_directory)

  grouped_cluster = (daisng_cluster_jan2017
                     .groupBy('daisngid')
                     .agg(collect_list('item')
                          .alias('items')
                         )
                     )

  def formatted_items(cluster_items):
      cluster_string = '|'.join(cluster_items)
      return cluster_string

  formatted_items_udf = udf(formatted_items, StringType())

  grouped_cluster.show()
      
  changed_grouped_cluster = (grouped_cluster
                             .select('daisngid', formatted_items_udf('items')
                                     .alias('items_string')
                             )
                           )

  changed_grouped_cluster.show()
  outdirectory = '/tmp/spark_data'
  #quick_delete(outdirectory)
  (changed_grouped_cluster
   .write
   .format("com.databricks.spark.csv")
   .option("delimiter", "\t")
   .mode("overwrite")
   .save(outdirectory)
  )


  # os.system('cat {}/*'.format(outdirectory))
  quick_print_directory(outdirectory)


#+END_SRC

#+RESULTS:
#+begin_example
+--------+-----+
|daisngid| item|
+--------+-----+
|       A|01-01|
|       A|02-02|
|       A|03-03|
|       A|04-04|
|       B|05-05|
|       B|06-06|
|       B|07-07|
|       B|08-08|
|       C|10-10|
|       C|11-11|
|       Z|50-50|
+--------+-----+

printing  /tmp/save_file_test
...printing /tmp/save_file_test/_SUCCESS

...printing /tmp/save_file_test/part-00000
Row(daisngid=u'A', item=u'01-01')
Row(daisngid=u'A', item=u'02-02')
Row(daisngid=u'A', item=u'03-03')
Row(daisngid=u'A', item=u'04-04')
Row(daisngid=u'B', item=u'05-05')
Row(daisngid=u'B', item=u'06-06')
Row(daisngid=u'B', item=u'07-07')
Row(daisngid=u'B', item=u'08-08')
Row(daisngid=u'C', item=u'10-10')
Row(daisngid=u'C', item=u'11-11')
Row(daisngid=u'Z', item=u'50-50')

+--------+--------------------+
|daisngid|               items|
+--------+--------------------+
|       B|[05-05, 06-06, 07...|
|       C|      [10-10, 11-11]|
|       Z|             [50-50]|
|       A|[01-01, 02-02, 03...|
+--------+--------------------+

+--------+--------------------+
|daisngid|        items_string|
+--------+--------------------+
|       B|05-05|06-06|07-07...|
|       C|         10-10|11-11|
|       Z|               50-50|
|       A|01-01|02-02|03-03...|
+--------+--------------------+

printing  /tmp/spark_data
...printing /tmp/spark_data/part-00151-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00198-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00036-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00054-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00062-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00088-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00009-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00150-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00123-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00149-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00041-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00057-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00183-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00162-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00184-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00080-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00063-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00121-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00068-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00115-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00006-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00074-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00167-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00175-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00072-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00179-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00082-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00155-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00126-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00071-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00106-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv
A	01-01|02-02|03-03|04-04

...printing /tmp/spark_data/part-00152-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00002-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00118-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00197-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00077-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00172-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00132-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00165-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00103-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00136-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00137-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00154-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00187-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00032-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00085-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00191-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00102-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00143-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00043-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00033-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00090-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00159-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00192-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00095-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00098-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00078-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00065-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00190-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00125-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00110-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00146-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00131-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00130-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00053-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00185-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00141-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/_SUCCESS

...printing /tmp/spark_data/part-00023-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00113-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00091-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00030-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00129-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00188-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00070-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00001-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00114-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00022-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00133-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00153-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00000-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00010-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00194-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00084-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00020-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00139-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00076-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00099-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00135-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00181-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00116-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00160-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00180-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00035-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00119-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00128-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00195-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00186-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00038-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00166-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00100-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00075-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00060-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00003-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00037-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00024-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00196-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00199-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00018-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00170-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00157-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00017-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00145-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00122-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00011-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00177-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00112-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00059-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00013-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00056-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00193-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00004-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00092-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00156-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00066-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00047-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00138-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00067-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00079-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00094-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00081-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00046-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00142-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00015-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00058-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00117-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00148-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00174-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00016-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00069-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00171-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00140-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00096-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00173-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00105-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00127-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00064-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00039-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00134-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00161-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00025-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00108-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00124-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00028-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00034-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00061-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00052-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00109-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00147-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00048-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00158-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00086-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00087-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00040-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00104-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv
Z	50-50

...printing /tmp/spark_data/part-00042-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00169-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00120-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00044-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00107-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00163-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00176-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00055-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00045-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00164-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00101-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00073-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00051-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00012-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00026-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00097-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00021-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00089-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv
C	10-10|11-11

...printing /tmp/spark_data/part-00168-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00031-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00144-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00083-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00007-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00050-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00005-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00111-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00029-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00189-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00093-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00027-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00019-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00178-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00182-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00008-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

...printing /tmp/spark_data/part-00049-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv
B	05-05|06-06|07-07|08-08

...printing /tmp/spark_data/part-00014-4cee4b83-52cf-4eeb-91ac-21b07329706e.csv

#+end_example

* Help docstring of rdd class
  
Help on RDD in module pyspark.rdd object:

class RDD(__builtin__.object)
 | A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.                                                                                         |
 | Represents an immutable, partitioned collection of elements that can be                                                                                        |
 | operated on in parallel.                                                                                                                                       |
 |                                                                                                                                                                |
 | Methods defined here:                                                                                                                                          |
 |                                                                                                                                                                |
 | __add__(self, other)                                                                                                                                           |
 | Return the union of this RDD and another one.                                                                                                                  |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([1, 1, 2, 3])                                                                                                                         |
 | >>> (rdd + rdd).collect()                                                                                                                                      |
 | [1, 1, 2, 3, 1, 1, 2, 3]                                                                                                                                       |
 |                                                                                                                                                                |
 | __getnewargs__(self)                                                                                                                                           |
 |                                                                                                                                                                |
 | __init__(self, jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))                                                                         |
 |                                                                                                                                                                |
 | __repr__(self)                                                                                                                                                 |
 |                                                                                                                                                                |
 | aggregate(self, zeroValue, seqOp, combOp)                                                                                                                      |
 | Aggregate the elements of each partition, and then the results for all                                                                                         |
 | the partitions, using a given combine functions and a neutral "zero                                                                                            |
 | value."                                                                                                                                                        |
 |                                                                                                                                                                |
 | The functions C{op(t1, t2)} is allowed to modify C{t1} and return it                                                                                           |
 | as its result value to avoid object allocation; however, it should not                                                                                         |
 | modify C{t2}.                                                                                                                                                  |
 |                                                                                                                                                                |
 | The first function (seqOp) can return a different result type, U, than                                                                                         |
 | the type of this RDD. Thus, we need one operation for merging a T into                                                                                         |
 | an U and one operation for merging two U                                                                                                                       |
 |                                                                                                                                                                |
 | >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))                                                                                                                |
 | >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))                                                                                                         |
 | >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)                                                                                              |
 | (10, 4)                                                                                                                                                        |
 | >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)                                                                                                        |
 | (0, 0)                                                                                                                                                         |
 |                                                                                                                                                                |
 | aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=<function portable_hash>)                                                 |
 | Aggregate the values of each key, using given combine functions and a neutral                                                                                  |
 | "zero value". This function can return a different result type, U, than the type                                                                               |
 | of the values in this RDD, V. Thus, we need one operation for merging a V into                                                                                 |
 | a U and one operation for merging two U's, The former operation is used for merging                                                                            |
 | values within a partition, and the latter is used for merging values between                                                                                   |
 | partitions. To avoid memory allocation, both of these functions are                                                                                            |
 | allowed to modify and return their first argument instead of creating a new U.                                                                                 |
 |                                                                                                                                                                |
 | cache(self)                                                                                                                                                    |
 | Persist this RDD with the default storage level (C{MEMORY_ONLY}).                                                                                              |
 |                                                                                                                                                                |
 | cartesian(self, other)                                                                                                                                         |
 | Return the Cartesian product of this RDD and another one, that is, the                                                                                         |
 | RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and                                                                                            |
 | C{b} is in C{other}.                                                                                                                                           |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([1, 2])                                                                                                                               |
 | >>> sorted(rdd.cartesian(rdd).collect())                                                                                                                       |
 | [(1, 1), (1, 2), (2, 1), (2, 2)]                                                                                                                               |
 |                                                                                                                                                                |
 | checkpoint(self)                                                                                                                                               |
 | Mark this RDD for checkpointing. It will be saved to a file inside the                                                                                         |
 | checkpoint directory set with L{SparkContext.setCheckpointDir()} and                                                                                           |
 | all references to its parent RDDs will be removed. This function must                                                                                          |
 | be called before any job has been executed on this RDD. It is strongly                                                                                         |
 | recommended that this RDD is persisted in memory, otherwise saving it                                                                                          |
 | on a file will require recomputation.                                                                                                                          |
 |                                                                                                                                                                |
 | coalesce(self, numPartitions, shuffle=False)                                                                                                                   |
 | Return a new RDD that is reduced into `numPartitions` partitions.                                                                                              |
 |                                                                                                                                                                |
 | >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()                                                                                                        |
 | [[1], [2, 3], [4, 5]]                                                                                                                                          |
 | >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()                                                                                            |
 | [[1, 2, 3, 4, 5]]                                                                                                                                                  |
 |                                                                                                                                                                |
 | cogroup(self, other, numPartitions=None)                                                                                                                       |
 | For each key k in C{self} or C{other}, return a resulting RDD that                                                                                             |
 | contains a tuple with the list of values for that key in C{self} as                                                                                            |
 | well as C{other}.                                                                                                                                              |
 |                                                                                                                                                                |
 | >>> x = sc.parallelize([("a", 1), ("b", 4)])                                                                                                                   |
 | >>> y = sc.parallelize([("a", 2)])                                                                                                                             |
 | >>> [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]                                                                                |
 | [('a', ([1], [2])), ('b', ([4], []))]                                                                                                                          |
 |                                                                                                                                                                |
 | collect(self)                                                                                                                                                  |
 | Return a list that contains all of the elements in this RDD.                                                                                                   |
 | Note that this method should only be used if the resulting array is expected                                                                                   |
 | to be small, as all the data is loaded into the driver's memory.                                                                                               |
 |                                                                                                                                                                |
 | collectAsMap(self)                                                                                                                                             |
 | Return the key-value pairs in this RDD to the master as a dictionary.                                                                                          |
 |                                                                                                                                                                |
 | Note that this method should only be used if the resulting data is expected                                                                                    |
 | to be small, as all the data is loaded into the driver's memory.                                                                                               |
 |                                                                                                                                                                |
 | >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()                                                                                                        |
 | >>> m[1]                                                                                                                                                       |
 | 2                                                                                                                                                              |
 | >>> m[3]                                                                                                                                                       |
 | 4                                                                                                                                                              |
 |                                                                                                                                                                |
 | combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<function portable_hash>)                                     |
 | Generic function to combine the elements for each key using a custom                                                                                           |
 | set of aggregation functions.                                                                                                                                  |
 |                                                                                                                                                                |
 | Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a "combined                                                                                        |
 | type" C.  Note that V and C can be different -- for example, one might                                                                                         |
 | group an RDD of type (Int, Int) into an RDD of type (Int, List[Int]).                                                                                          |
 |                                                                                                                                                                |
 | Users provide three functions:                                                                                                                                 |
 |                                                                                                                                                                |
 | - C{createCombiner}, which turns a V into a C (e.g., creates                                                                                                   |
 | a one-element list)                                                                                                                                            |
 | - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of                                                                                            |
 | a list)                                                                                                                                                        |
 | - C{mergeCombiners}, to combine two C's into a single one.                                                                                                     |
 |                                                                                                                                                                |
 | In addition, users can control the partitioning of the output RDD.                                                                                             |
 |                                                                                                                                                                |
 | >>> x = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])                                                                                                         |
 | >>> def add(a, b): return a + str(b)                                                                                                                           |
 | >>> sorted(x.combineByKey(str, add, add).collect())                                                                                                            |
 | [('a', '11'), ('b', '1')]                                                                                                                                      |
 |                                                                                                                                                                |
 | count(self)                                                                                                                                                    |
 | Return the number of elements in this RDD.                                                                                                                     |
 |                                                                                                                                                                |
 | >>> sc.parallelize([2, 3, 4]).count()                                                                                                                          |
 | 3                                                                                                                                                              |
 |                                                                                                                                                                |
 | countApprox(self, timeout, confidence=0.95)                                                                                                                    |
 | .. note:: Experimental                                                                                                                                         |
 |                                                                                                                                                                |
 | Approximate version of count() that returns a potentially incomplete                                                                                           |
 | result within a timeout, even if not all tasks have finished.                                                                                                  |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize(range(1000), 10)                                                                                                                      |
 | >>> rdd.countApprox(1000, 1.0)                                                                                                                                 |
 | 1000                                                                                                                                                           |
 |                                                                                                                                                                |
 | countApproxDistinct(self, relativeSD=0.05)                                                                                                                     |
 | .. note:: Experimental                                                                                                                                         |
 |                                                                                                                                                                |
 | Return approximate number of distinct elements in the RDD.                                                                                                     |
 |                                                                                                                                                                |
 | The algorithm used is based on streamlib's implementation of                                                                                                   |
 | `"HyperLogLog in Practice: Algorithmic Engineering of a State                                                                                                  |
 | of The Art Cardinality Estimation Algorithm", available here                                                                                                   |
 | <http://dx.doi.org/10.1145/2452376.2452456>`_.                                                                                                                 |
 |                                                                                                                                                                |
 | :param relativeSD: Relative accuracy. Smaller values create                                                                                                    |
 | counters that require more space.                                                                                                                              |
 | It must be greater than 0.000017.                                                                                                                              |
 |                                                                                                                                                                |
 | >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()                                                                                             |
 | >>> 900 < n < 1100                                                                                                                                             |
 | True                                                                                                                                                           |
 | >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()                                                                                    |
 | >>> 16 < n < 24                                                                                                                                                |
 | True                                                                                                                                                           |
 |                                                                                                                                                                |
 | countByKey(self)                                                                                                                                               |
 | Count the number of elements for each key, and return the result to the                                                                                        |
 | master as a dictionary.                                                                                                                                        |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])                                                                                                       |
 | >>> sorted(rdd.countByKey().items())                                                                                                                           |
 | [('a', 2), ('b', 1)]                                                                                                                                           |
 |                                                                                                                                                                |
 | countByValue(self)                                                                                                                                             |
 | Return the count of each unique value in this RDD as a dictionary of                                                                                           |
 | (value, count) pairs.                                                                                                                                          |
 |                                                                                                                                                                |
 | >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())                                                                                          |
 | [(1, 2), (2, 3)]                                                                                                                                               |
 |                                                                                                                                                                |
 | distinct(self, numPartitions=None)                                                                                                                             |
 | Return a new RDD containing the distinct elements in this RDD.                                                                                                 |
 |                                                                                                                                                                |
 | >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())                                                                                                  |
 | [1, 2, 3]                                                                                                                                                      |
 |                                                                                                                                                                |
 | filter(self, f)                                                                                                                                                |
 | Return a new RDD containing only the elements that satisfy a predicate.                                                                                        |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([1, 2, 3, 4, 5])                                                                                                                      |
 | >>> rdd.filter(lambda x: x % 2 == 0).collect()                                                                                                                 |
 | [2, 4]                                                                                                                                                         |
 |                                                                                                                                                                |
 | first(self)                                                                                                                                                    |
 | Return the first element in this RDD.                                                                                                                          |
 |                                                                                                                                                                |
 | >>> sc.parallelize([2, 3, 4]).first()                                                                                                                          |
 | 2                                                                                                                                                              |
 | >>> sc.parallelize([]).first()                                                                                                                                 |
 | Traceback (most recent call last):                                                                                                                             |
 | ...                                                                                                                                                            |
 | ValueError: RDD is empty                                                                                                                                       |
 |                                                                                                                                                                |
 | flatMap(self, f, preservesPartitioning=False)                                                                                                                  |
 | Return a new RDD by first applying a function to all elements of this                                                                                          |
 | RDD, and then flattening the results.                                                                                                                          |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([2, 3, 4])                                                                                                                            |
 | >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())                                                                                                       |
 | [1, 1, 1, 2, 2, 3]                                                                                                                                             |
 | >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())                                                                                                  |
 | [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]                                                                                                               |
 |                                                                                                                                                                |
 | flatMapValues(self, f)                                                                                                                                         |
 | Pass each value in the key-value pair RDD through a flatMap function                                                                                           |
 | without changing the keys; this also retains the original RDD's                                                                                                |
 | partitioning.                                                                                                                                                  |
 |                                                                                                                                                                |
 | >>> x = sc.parallelize([("a", ["x", "y", "z"]), ("b", ["p", "r"])])                                                                                            |
 | >>> def f(x): return x                                                                                                                                         |
 | >>> x.flatMapValues(f).collect()                                                                                                                               |
 | [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]                                                                                                   |
 |                                                                                                                                                                |
 | fold(self, zeroValue, op)                                                                                                                                      |
 | Aggregate the elements of each partition, and then the results for all                                                                                         |
 | the partitions, using a given associative function and a neutral "zero value."                                                                                 |
 |                                                                                                                                                                |
 | The function C{op(t1, t2)} is allowed to modify C{t1} and return it                                                                                            |
 | as its result value to avoid object allocation; however, it should not                                                                                         |
 | modify C{t2}.                                                                                                                                                  |
 |                                                                                                                                                                |
 | This behaves somewhat differently from fold operations implemented                                                                                             |
 | for non-distributed collections in functional languages like Scala.                                                                                            |
 | This fold operation may be applied to partitions individually, and then                                                                                        |
 | fold those results into the final result, rather than apply the fold                                                                                           |
 | to each element sequentially in some defined ordering. For functions                                                                                           |
 | that are not commutative, the result may differ from that of a fold                                                                                            |
 | applied to a non-distributed collection.                                                                                                                       |
 |                                                                                                                                                                |
 | >>> from operator import add                                                                                                                                   |
 | >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)                                                                                                               |
 | 15                                                                                                                                                             |
 |                                                                                                                                                                |
 | foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=<function portable_hash>)                                                                   |
 | Merge the values for each key using an associative function "func"                                                                                             |
 | and a neutral "zeroValue" which may be added to the result an                                                                                                  |
 | arbitrary number of times, and must not change the result                                                                                                      |
 | (e.g., 0 for addition, or 1 for multiplication.).                                                                                                              |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])                                                                                                       |
 | >>> from operator import add                                                                                                                                   |
 | >>> sorted(rdd.foldByKey(0, add).collect())                                                                                                                    |
 | [('a', 2), ('b', 1)]                                                                                                                                           |
 |                                                                                                                                                                |
 | foreach(self, f)                                                                                                                                               |
 | Applies a function to all elements of this RDD.                                                                                                                |
 |                                                                                                                                                                |
 | >>> def f(x): print(x)                                                                                                                                         |
 | >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)                                                                                                                 |
 |                                                                                                                                                                |
 | foreachPartition(self, f)                                                                                                                                      |
 | Applies a function to each partition of this RDD.                                                                                                              |
 |                                                                                                                                                                |
 | >>> def f(iterator):                                                                                                                                           |
 | ...      for x in iterator:                                                                                                                                    |
 | ...           print(x)                                                                                                                                         |
 | >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)                                                                                                        |
 |                                                                                                                                                                |
 | fullOuterJoin(self, other, numPartitions=None)                                                                                                                 |
 | Perform a right outer join of C{self} and C{other}.                                                                                                            |
 |                                                                                                                                                                |
 | For each element (k, v) in C{self}, the resulting RDD will either                                                                                              |
 | contain all pairs (k, (v, w)) for w in C{other}, or the pair                                                                                                   |
 | (k, (v, None)) if no elements in C{other} have key k.                                                                                                          |
 |                                                                                                                                                                |
 | Similarly, for each element (k, w) in C{other}, the resulting RDD will                                                                                         |
 | either contain all pairs (k, (v, w)) for v in C{self}, or the pair                                                                                             |
 | (k, (None, w)) if no elements in C{self} have key k.                                                                                                           |
 |                                                                                                                                                                |
 | Hash-partitions the resulting RDD into the given number of partitions.                                                                                         |
 |                                                                                                                                                                |
 | >>> x = sc.parallelize([("a", 1), ("b", 4)])                                                                                                                   |
 | >>> y = sc.parallelize([("a", 2), ("c", 8)])                                                                                                                   |
 | >>> sorted(x.fullOuterJoin(y).collect())                                                                                                                       |
 | [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]                                                                                                            |
 |                                                                                                                                                                |
 | getCheckpointFile(self)                                                                                                                                        |
 | Gets the name of the file to which this RDD was checkpointed                                                                                                   |
 |                                                                                                                                                                |
 | getNumPartitions(self)                                                                                                                                         |
 | Returns the number of partitions in RDD                                                                                                                        |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([1, 2, 3, 4], 2)                                                                                                                      |
 | >>> rdd.getNumPartitions()                                                                                                                                     |
 | 2                                                                                                                                                              |
 |                                                                                                                                                                |
 | getStorageLevel(self)                                                                                                                                          |
 | Get the RDD's current storage level.                                                                                                                           |
 |                                                                                                                                                                |
 | >>> rdd1 = sc.parallelize([1,2])                                                                                                                               |
 | >>> rdd1.getStorageLevel()                                                                                                                                     |
 | StorageLevel(False, False, False, False, 1)                                                                                                                    |
 | >>> print(rdd1.getStorageLevel())                                                                                                                              |
 | Serialized 1x Replicated                                                                                                                                       |
 |                                                                                                                                                                |
 | glom(self)                                                                                                                                                     |
 | Return an RDD created by coalescing all elements within each partition                                                                                         |
 | into a list.                                                                                                                                                   |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([1, 2, 3, 4], 2)                                                                                                                      |
 | >>> sorted(rdd.glom().collect())                                                                                                                               |
 | [[1, 2], [3, 4]]                                                                                                                                               |
 |                                                                                                                                                                |
 | groupBy(self, f, numPartitions=None, partitionFunc=<function portable_hash>)                                                                                   |
 | Return an RDD of grouped items.                                                                                                                                |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])                                                                                                                   |
 | >>> result = rdd.groupBy(lambda x: x % 2).collect()                                                                                                            |
 | >>> sorted([(x, sorted(y)) for (x, y) in result])                                                                                                              |
 | [(0, [2, 8]), (1, [1, 1, 3, 5])]                                                                                                                               |
 |                                                                                                                                                                |
 | groupByKey(self, numPartitions=None, partitionFunc=<function portable_hash>)                                                                                   |
 | Group the values for each key in the RDD into a single sequence.                                                                                               |
 | Hash-partitions the resulting RDD with numPartitions partitions.                                                                                               |
 |                                                                                                                                                                |
 | Note: If you are grouping in order to perform an aggregation (such as a                                                                                        |
 | sum or average) over each key, using reduceByKey or aggregateByKey will                                                                                        |
 | provide much better performance.                                                                                                                               |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])                                                                                                       |
 | >>> sorted(rdd.groupByKey().mapValues(len).collect())                                                                                                          |
 | [('a', 2), ('b', 1)]                                                                                                                                           |
 | >>> sorted(rdd.groupByKey().mapValues(list).collect())                                                                                                         |
 | [('a', [1, 1]), ('b', [1])]                                                                                                                                    |
 |                                                                                                                                                                |
 | groupWith(self, other, *others)                                                                                                                                |
 | Alias for cogroup but with support for multiple RDDs.                                                                                                          |
 |                                                                                                                                                                |
 | >>> w = sc.parallelize([("a", 5), ("b", 6)])                                                                                                                   |
 | >>> x = sc.parallelize([("a", 1), ("b", 4)])                                                                                                                   |
 | >>> y = sc.parallelize([("a", 2)])                                                                                                                             |
 | >>> z = sc.parallelize([("b", 42)])                                                                                                                            |
 | >>> [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]                                                                        |
 | [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]                                                                                                      |
 |                                                                                                                                                                |
 | histogram(self, buckets)                                                                                                                                       |
 | Compute a histogram using the provided buckets. The buckets                                                                                                    |
 | are all open to the right except for the last which is closed.                                                                                                 |
 | e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],                                                                                                |
 | which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1                                                                                                |
 | and 50 we would have a histogram of 1,0,1.                                                                                                                     |
 |                                                                                                                                                                |
 | If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),                                                                                                     |
 | this can be switched from an O(log n) inseration to O(1) per                                                                                                   |
 | element (where n is the number of buckets).                                                                                                                    |
 |                                                                                                                                                                |
 | Buckets must be sorted, not contain any duplicates, and have                                                                                                   |
 | at least two elements.                                                                                                                                         |
 |                                                                                                                                                                |
 | If `buckets` is a number, it will generate buckets which are                                                                                                   |
 | evenly spaced between the minimum and maximum of the RDD. For                                                                                                  |
 | example, if the min value is 0 and the max is 100, given `buckets`                                                                                             |
 | as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must                                                                                            |
 | be at least 1. An exception is raised if the RDD contains infinity.                                                                                            |
 | If the elements in the RDD do not vary (max == min), a single bucket                                                                                           |
 | will be used.                                                                                                                                                  |
 |                                                                                                                                                                |
 | The return value is a tuple of buckets and histogram.                                                                                                          |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize(range(51))                                                                                                                            |
 | >>> rdd.histogram(2)                                                                                                                                           |
 | ([0, 25, 50], [25, 26])                                                                                                                                        |
 | >>> rdd.histogram([0, 5, 25, 50])                                                                                                                              |
 | ([0, 5, 25, 50], [5, 20, 26])                                                                                                                                  |
 | >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets                                                                                                |
 | ([0, 15, 30, 45, 60], [15, 15, 15, 6])                                                                                                                         |
 | >>> rdd = sc.parallelize(["ab", "ac", "b", "bd", "ef"])                                                                                                        |
 | >>> rdd.histogram(("a", "b", "c"))                                                                                                                             |
 | (('a', 'b', 'c'), [2, 2])                                                                                                                                      |
 |                                                                                                                                                                |
 | id(self)                                                                                                                                                       |
 | A unique ID for this RDD (within its SparkContext).                                                                                                            |
 |                                                                                                                                                                |
 | intersection(self, other)                                                                                                                                      |
 | Return the intersection of this RDD and another one. The output will                                                                                           |
 | not contain any duplicate elements, even if the input RDDs did.                                                                                                |
 |                                                                                                                                                                |
 | Note that this method performs a shuffle internally.                                                                                                           |
 |                                                                                                                                                                |
 | >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])                                                                                                                 |
 | >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])                                                                                                                  |
 | >>> rdd1.intersection(rdd2).collect()                                                                                                                          |
 | [1, 2, 3]                                                                                                                                                      |
 |                                                                                                                                                                |
 | isCheckpointed(self)                                                                                                                                           |
 | Return whether this RDD has been checkpointed or not                                                                                                           |
 |                                                                                                                                                                |
 | isEmpty(self)                                                                                                                                                  |
 | Returns true if and only if the RDD contains no elements at all. Note that an RDD                                                                              |
 | may be empty even when it has at least 1 partition.                                                                                                            |
 |                                                                                                                                                                |
 | >>> sc.parallelize([]).isEmpty()                                                                                                                               |
 | True                                                                                                                                                           |
 | >>> sc.parallelize([1]).isEmpty()                                                                                                                              |
 | False                                                                                                                                                          |
 |                                                                                                                                                                |
 | join(self, other, numPartitions=None)                                                                                                                          |
 | Return an RDD containing all pairs of elements with matching keys in                                                                                           |
 | C{self} and C{other}.                                                                                                                                          |
 |                                                                                                                                                                |
 | Each pair of elements will be returned as a (k, (v1, v2)) tuple, where                                                                                         |
 | (k, v1) is in C{self} and (k, v2) is in C{other}.                                                                                                              |
 |                                                                                                                                                                |
 | Performs a hash join across the cluster.                                                                                                                       |
 |                                                                                                                                                                |
 | >>> x = sc.parallelize([("a", 1), ("b", 4)])                                                                                                                   |
 | >>> y = sc.parallelize([("a", 2), ("a", 3)])                                                                                                                   |
 | >>> sorted(x.join(y).collect())                                                                                                                                |
 | [('a', (1, 2)), ('a', (1, 3))]                                                                                                                                 |
 |                                                                                                                                                                |
 | keyBy(self, f)                                                                                                                                                 |
 | Creates tuples of the elements in this RDD by applying C{f}.                                                                                                   |
 |                                                                                                                                                                |
 | >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)                                                                                                        |
 | >>> y = sc.parallelize(zip(range(0,5), range(0,5)))                                                                                                            |
 | >>> [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]                                                                                       |
 | [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]                                                                            |
 |                                                                                                                                                                |
 | keys(self)                                                                                                                                                     |
 | Return an RDD with the keys of each tuple.                                                                                                                     |
 |                                                                                                                                                                |
 | >>> m = sc.parallelize([(1, 2), (3, 4)]).keys()                                                                                                                |
 | >>> m.collect()                                                                                                                                                |
 | [1, 3]                                                                                                                                                         |
 |                                                                                                                                                                |
 | leftOuterJoin(self, other, numPartitions=None)                                                                                                                 |
 | Perform a left outer join of C{self} and C{other}.                                                                                                             |
 |                                                                                                                                                                |
 | For each element (k, v) in C{self}, the resulting RDD will either                                                                                              |
 | contain all pairs (k, (v, w)) for w in C{other}, or the pair                                                                                                   |
 | (k, (v, None)) if no elements in C{other} have key k.                                                                                                          |
 |                                                                                                                                                                |
 | Hash-partitions the resulting RDD into the given number of partitions.                                                                                         |
 |                                                                                                                                                                |
 | >>> x = sc.parallelize([("a", 1), ("b", 4)])                                                                                                                   |
 | >>> y = sc.parallelize([("a", 2)])                                                                                                                             |
 | >>> sorted(x.leftOuterJoin(y).collect())                                                                                                                       |
 | [('a', (1, 2)), ('b', (4, None))]                                                                                                                              |
 |                                                                                                                                                                |
 | lookup(self, key)                                                                                                                                              |
 | Return the list of values in the RDD for key `key`. This operation                                                                                             |
 | is done efficiently if the RDD has a known partitioner by only                                                                                                 |
 | searching the partition that the key maps to.                                                                                                                  |
 |                                                                                                                                                                |
 | >>> l = range(1000)                                                                                                                                            |
 | >>> rdd = sc.parallelize(zip(l, l), 10)                                                                                                                        |
 | >>> rdd.lookup(42)  # slow                                                                                                                                     |
 | [42]                                                                                                                                                           |
 | >>> sorted = rdd.sortByKey()                                                                                                                                   |
 | >>> sorted.lookup(42)  # fast                                                                                                                                  |
 | [42]                                                                                                                                                           |
 | >>> sorted.lookup(1024)                                                                                                                                        |
 | []                                                                                                                                                             |
 | >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()                                                                                                    |
 | >>> list(rdd2.lookup(('a', 'b'))[0])                                                                                                                           |
 | ['c']                                                                                                                                                          |
 |                                                                                                                                                                |
 | map(self, f, preservesPartitioning=False)                                                                                                                      |
 | Return a new RDD by applying a function to each element of this RDD.                                                                                           |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize(["b", "a", "c"])                                                                                                                      |
 | >>> sorted(rdd.map(lambda x: (x, 1)).collect())                                                                                                                |
 | [('a', 1), ('b', 1), ('c', 1)]                                                                                                                                 |
 |                                                                                                                                                                |
 | mapPartitions(self, f, preservesPartitioning=False)                                                                                                            |
 | Return a new RDD by applying a function to each partition of this RDD.                                                                                         |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([1, 2, 3, 4], 2)                                                                                                                      |
 | >>> def f(iterator): yield sum(iterator)                                                                                                                       |
 | >>> rdd.mapPartitions(f).collect()                                                                                                                             |
 | [3, 7]                                                                                                                                                         |
 |                                                                                                                                                                |
 | mapPartitionsWithIndex(self, f, preservesPartitioning=False)                                                                                                   |
 | Return a new RDD by applying a function to each partition of this RDD,                                                                                         |
 | while tracking the index of the original partition.                                                                                                            |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([1, 2, 3, 4], 4)                                                                                                                      |
 | >>> def f(splitIndex, iterator): yield splitIndex                                                                                                              |
 | >>> rdd.mapPartitionsWithIndex(f).sum()                                                                                                                        |
 | 6                                                                                                                                                              |
 |                                                                                                                                                                |
 | mapPartitionsWithSplit(self, f, preservesPartitioning=False)                                                                                                   |
 | Deprecated: use mapPartitionsWithIndex instead.                                                                                                                |
 |                                                                                                                                                                |
 | Return a new RDD by applying a function to each partition of this RDD,                                                                                         |
 | while tracking the index of the original partition.                                                                                                            |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([1, 2, 3, 4], 4)                                                                                                                      |
 | >>> def f(splitIndex, iterator): yield splitIndex                                                                                                              |
 | >>> rdd.mapPartitionsWithSplit(f).sum()                                                                                                                        |
 | 6                                                                                                                                                              |
 |                                                                                                                                                                |
 | mapValues(self, f)                                                                                                                                             |
 | Pass each value in the key-value pair RDD through a map function                                                                                               |
 | without changing the keys; this also retains the original RDD's                                                                                                |
 | partitioning.                                                                                                                                                  |
 |                                                                                                                                                                |
 | >>> x = sc.parallelize([("a", ["apple", "banana", "lemon"]), ("b", ["grapes"])])                                                                               |
 | >>> def f(x): return len(x)                                                                                                                                    |
 | >>> x.mapValues(f).collect()                                                                                                                                   |
 | [('a', 3), ('b', 1)]                                                                                                                                           |
 |                                                                                                                                                                |
 | max(self, key=None)                                                                                                                                            |
 | Find the maximum item in this RDD.                                                                                                                             |
 |                                                                                                                                                                |
 | :param key: A function used to generate key for comparing                                                                                                      |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])                                                                                                               |
 | >>> rdd.max()                                                                                                                                                  |
 | 43.0                                                                                                                                                           |
 | >>> rdd.max(key=str)                                                                                                                                           |
 | 5.0                                                                                                                                                            |
 |                                                                                                                                                                |
 | mean(self)                                                                                                                                                     |
 | Compute the mean of this RDD's elements.                                                                                                                       |
 |                                                                                                                                                                |
 | >>> sc.parallelize([1, 2, 3]).mean()                                                                                                                           |
 | 2.0                                                                                                                                                            |
 |                                                                                                                                                                |
 | meanApprox(self, timeout, confidence=0.95)                                                                                                                     |
 | .. note:: Experimental                                                                                                                                         |
 |                                                                                                                                                                |
 | Approximate operation to return the mean within a timeout                                                                                                      |
 | or meet the confidence.                                                                                                                                        |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize(range(1000), 10)                                                                                                                      |
 | >>> r = sum(range(1000)) / 1000.0                                                                                                                              |
 | >>> abs(rdd.meanApprox(1000) - r) / r < 0.05                                                                                                                   |
 | True                                                                                                                                                           |
 |                                                                                                                                                                |
 | min(self, key=None)                                                                                                                                            |
 | Find the minimum item in this RDD.                                                                                                                             |
 |                                                                                                                                                                |
 | :param key: A function used to generate key for comparing                                                                                                      |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])                                                                                                               |
 | >>> rdd.min()                                                                                                                                                  |
 | 2.0                                                                                                                                                            |
 | >>> rdd.min(key=str)                                                                                                                                           |
 | 10.0                                                                                                                                                           |
 |                                                                                                                                                                |
 | name(self)                                                                                                                                                     |
 | Return the name of this RDD.                                                                                                                                   |
 |                                                                                                                                                                |
 | partitionBy(self, numPartitions, partitionFunc=<function portable_hash>)                                                                                       |
 | Return a copy of the RDD partitioned using the specified partitioner.                                                                                          |
 |                                                                                                                                                                |
 | >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))                                                                                        |
 | >>> sets = pairs.partitionBy(2).glom().collect()                                                                                                               |
 | >>> len(set(sets[0]).intersection(set(sets[1])))                                                                                                               |
 | 0                                                                                                                                                              |
 |                                                                                                                                                                |
 | persist(self, storageLevel=StorageLevel(False, True, False, False, 1))                                                                                         |
 | Set this RDD's storage level to persist its values across operations                                                                                           |
 | after the first time it is computed. This can only be used to assign                                                                                           |
 | a new storage level if the RDD does not have a storage level set yet.                                                                                          |
 | If no storage level is specified defaults to (C{MEMORY_ONLY}).                                                                                                 |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize(["b", "a", "c"])                                                                                                                      |
 | >>> rdd.persist().is_cached                                                                                                                                    |
 | True                                                                                                                                                           |
 |                                                                                                                                                                |
 | pipe(self, command, env=None, checkCode=False)                                                                                                                 |
 | Return an RDD created by piping elements to a forked external process.                                                                                         |
 |                                                                                                                                                                |
 | >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()                                                                                                  |
 | [u'1', u'2', u'', u'3']                                                                                                                                        |
 |                                                                                                                                                                |
 | :param checkCode: whether or not to check the return value of the shell command.                                                                               |
 |                                                                                                                                                                |
 | randomSplit(self, weights, seed=None)                                                                                                                          |
 | Randomly splits this RDD with the provided weights.                                                                                                            |
 |                                                                                                                                                                |
 | :param weights: weights for splits, will be normalized if they don't sum to 1                                                                                  |
 | :param seed: random seed                                                                                                                                       |
 | :return: split RDDs in a list                                                                                                                                  |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize(range(500), 1)                                                                                                                        |
 | >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)                                                                                                                   |
 | >>> len(rdd1.collect() + rdd2.collect())                                                                                                                       |
 | 500                                                                                                                                                            |
 | >>> 150 < rdd1.count() < 250                                                                                                                                   |
 | True                                                                                                                                                           |
 | >>> 250 < rdd2.count() < 350                                                                                                                                   |
 | True                                                                                                                                                           |
 |                                                                                                                                                                |
 | reduce(self, f)                                                                                                                                                |
 | Reduces the elements of this RDD using the specified commutative and                                                                                           |
 | associative binary operator. Currently reduces partitions locally.                                                                                             |
 |                                                                                                                                                                |
 | >>> from operator import add                                                                                                                                   |
 | >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)                                                                                                                |
 | 15                                                                                                                                                             |
 | >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)                                                                                |
 | 10                                                                                                                                                             |
 | >>> sc.parallelize([]).reduce(add)                                                                                                                             |
 | Traceback (most recent call last):                                                                                                                             |
 | ...                                                                                                                                                            |
 | ValueError: Can not reduce() empty RDD                                                                                                                         |
 |                                                                                                                                                                |
 | reduceByKey(self, func, numPartitions=None, partitionFunc=<function portable_hash>)                                                                            |
 | Merge the values for each key using an associative and commutative reduce function.                                                                            |
 |                                                                                                                                                                |
 | This will also perform the merging locally on each mapper before                                                                                               |
 | sending results to a reducer, similarly to a "combiner" in MapReduce.                                                                                          |
 |                                                                                                                                                                |
 | Output will be partitioned with C{numPartitions} partitions, or                                                                                                |
 | the default parallelism level if C{numPartitions} is not specified.                                                                                            |
 | Default partitioner is hash-partition.                                                                                                                         |
 |                                                                                                                                                                |
 | >>> from operator import add                                                                                                                                   |
 | >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])                                                                                                       |
 | >>> sorted(rdd.reduceByKey(add).collect())                                                                                                                     |
 | [('a', 2), ('b', 1)]                                                                                                                                           |
 |                                                                                                                                                                |
 | reduceByKeyLocally(self, func)                                                                                                                                 |
 | Merge the values for each key using an associative and commutative reduce function, but                                                                        |
 | return the results immediately to the master as a dictionary.                                                                                                  |
 |                                                                                                                                                                |
 | This will also perform the merging locally on each mapper before                                                                                               |
 | sending results to a reducer, similarly to a "combiner" in MapReduce.                                                                                          |
 |                                                                                                                                                                |
 | >>> from operator import add                                                                                                                                   |
 | >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])                                                                                                       |
 | >>> sorted(rdd.reduceByKeyLocally(add).items())                                                                                                                |
 | [('a', 2), ('b', 1)]                                                                                                                                           |
 |                                                                                                                                                                |
 | repartition(self, numPartitions)                                                                                                                               |
 | Return a new RDD that has exactly numPartitions partitions.                                                                                                    |
 |                                                                                                                                                                |
 | Can increase or decrease the level of parallelism in this RDD.                                                                                                 |
 | Internally, this uses a shuffle to redistribute data.                                                                                                          |
 | If you are decreasing the number of partitions in this RDD, consider                                                                                           |
 | using `coalesce`, which can avoid performing a shuffle.                                                                                                        |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)                                                                                                                   |
 | >>> sorted(rdd.glom().collect())                                                                                                                               |
 | [[1], [2, 3], [4, 5], [6, 7]]                                                                                                                                  |
 | >>> len(rdd.repartition(2).glom().collect())                                                                                                                   |
 | 2                                                                                                                                                              |
 | >>> len(rdd.repartition(10).glom().collect())                                                                                                                  |
 | 10                                                                                                                                                             |
 |                                                                                                                                                                |
 | repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=<function portable_hash>, ascending=True, keyfunc=<function <lambda>>)              |
 | Repartition the RDD according to the given partitioner and, within each resulting partition,                                                                   |
 | sort records by their keys.                                                                                                                                    |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])                                                                                     |
 | >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, 2)                                                                                       |
 | >>> rdd2.glom().collect()                                                                                                                                      |
 | [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]                                                                                                           |
 |                                                                                                                                                                |
 | rightOuterJoin(self, other, numPartitions=None)                                                                                                                |
 | Perform a right outer join of C{self} and C{other}.                                                                                                            |
 |                                                                                                                                                                |
 | For each element (k, w) in C{other}, the resulting RDD will either                                                                                             |
 | contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))                                                                                        |
 | if no elements in C{self} have key k.                                                                                                                          |
 |                                                                                                                                                                |
 | Hash-partitions the resulting RDD into the given number of partitions.                                                                                         |
 |                                                                                                                                                                |
 | >>> x = sc.parallelize([("a", 1), ("b", 4)])                                                                                                                   |
 | >>> y = sc.parallelize([("a", 2)])                                                                                                                             |
 | >>> sorted(y.rightOuterJoin(x).collect())                                                                                                                      |
 | [('a', (2, 1)), ('b', (None, 4))]                                                                                                                              |
 |                                                                                                                                                                |
 | sample(self, withReplacement, fraction, seed=None)                                                                                                             |
 | Return a sampled subset of this RDD.                                                                                                                           |
 |                                                                                                                                                                |
 | :param withReplacement: can elements be sampled multiple times (replaced when sampled out)                                                                     |
 | :param fraction: expected size of the sample as a fraction of this RDD's size                                                                                  |
 | without replacement: probability that each element is chosen; fraction must be [0, 1]                                                                          |
 | with replacement: expected number of times each element is chosen; fraction must be >= 0                                                                       |
 | :param seed: seed for the random number generator                                                                                                              |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize(range(100), 4)                                                                                                                        |
 | >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14                                                                                                              |
 | True                                                                                                                                                           |
 |                                                                                                                                                                |
 | sampleByKey(self, withReplacement, fractions, seed=None)                                                                                                       |
 | Return a subset of this RDD sampled by key (via stratified sampling).                                                                                          |
 | Create a sample of this RDD using variable sampling rates for                                                                                                  |
 | different keys as specified by fractions, a key to sampling rate map.                                                                                          |
 |                                                                                                                                                                |
 | >>> fractions = {"a": 0.2, "b": 0.1}                                                                                                                           |
 | >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))                                                                           |
 | >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())                                                                                 |
 | >>> 100 < len(sample["a"]) < 300 and 50 < len(sample["b"]) < 150                                                                                               |
 | True                                                                                                                                                           |
 | >>> max(sample["a"]) <= 999 and min(sample["a"]) >= 0                                                                                                          |
 | True                                                                                                                                                           |
 | >>> max(sample["b"]) <= 999 and min(sample["b"]) >= 0                                                                                                          |
 | True                                                                                                                                                           |
 |                                                                                                                                                                |
 | sampleStdev(self)                                                                                                                                              |
 | Compute the sample standard deviation of this RDD's elements (which                                                                                            |
 | corrects for bias in estimating the standard deviation by dividing by                                                                                          |
 | N-1 instead of N).                                                                                                                                             |
 |                                                                                                                                                                |
 | >>> sc.parallelize([1, 2, 3]).sampleStdev()                                                                                                                    |
 | 1.0                                                                                                                                                            |
 |                                                                                                                                                                |
 | sampleVariance(self)                                                                                                                                           |
 | Compute the sample variance of this RDD's elements (which corrects                                                                                             |
 | for bias in estimating the variance by dividing by N-1 instead of N).                                                                                          |
 |                                                                                                                                                                |
 | >>> sc.parallelize([1, 2, 3]).sampleVariance()                                                                                                                 |
 | 1.0                                                                                                                                                            |
 |                                                                                                                                                                |
 | saveAsHadoopDataset(self, conf, keyConverter=None, valueConverter=None)                                                                                        |
 | Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file                                                                             |
 | system, using the old Hadoop OutputFormat API (mapred package). Keys/values are                                                                                |
 | converted for output using either user specified converters or, by default,                                                                                    |
 | L{org.apache.spark.api.python.JavaToWritableConverter}.                                                                                                        |
 |                                                                                                                                                                |
 | :param conf: Hadoop job configuration, passed in as a dict                                                                                                     |
 | :param keyConverter: (None by default)                                                                                                                         |
 | :param valueConverter: (None by default)                                                                                                                       |
 |                                                                                                                                                                |
 | saveAsHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None) |
 | Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file                                                                             |
 | system, using the old Hadoop OutputFormat API (mapred package). Key and value types                                                                            |
 | will be inferred if not specified. Keys and values are converted for output using either                                                                       |
 | user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The                                                                       |
 | C{conf} is applied on top of the base Hadoop conf associated with the SparkContext                                                                             |
 | of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.                                                                         |
 |                                                                                                                                                                |
 | :param path: path to Hadoop file                                                                                                                               |
 | :param outputFormatClass: fully qualified classname of Hadoop OutputFormat                                                                                     |
 | (e.g. "org.apache.hadoop.mapred.SequenceFileOutputFormat")                                                                                                     |
 | :param keyClass: fully qualified classname of key Writable class                                                                                               |
 | (e.g. "org.apache.hadoop.io.IntWritable", None by default)                                                                                                     |
 | :param valueClass: fully qualified classname of value Writable class                                                                                           |
 | (e.g. "org.apache.hadoop.io.Text", None by default)                                                                                                            |
 | :param keyConverter: (None by default)                                                                                                                         |
 | :param valueConverter: (None by default)                                                                                                                       |
 | :param conf: (None by default)                                                                                                                                 |
 | :param compressionCodecClass: (None by default)                                                                                                                |
 |                                                                                                                                                                |
 | saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None)                                                                                  |
 | Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file                                                                             |
 | system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are                                                                             |
 | converted for output using either user specified converters or, by default,                                                                                    |
 | L{org.apache.spark.api.python.JavaToWritableConverter}.                                                                                                        |
 |                                                                                                                                                                |
 | :param conf: Hadoop job configuration, passed in as a dict                                                                                                     |
 | :param keyConverter: (None by default)                                                                                                                         |
 | :param valueConverter: (None by default)                                                                                                                       |
 |                                                                                                                                                                |
 | saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)                       |
 | Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file                                                                             |
 | system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types                                                                         |
 | will be inferred if not specified. Keys and values are converted for output using either                                                                       |
 | user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The                                                                       |
 | C{conf} is applied on top of the base Hadoop conf associated with the SparkContext                                                                             |
 | of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.                                                                         |
 |                                                                                                                                                                |
 | :param path: path to Hadoop file                                                                                                                               |
 | :param outputFormatClass: fully qualified classname of Hadoop OutputFormat                                                                                     |
 | (e.g. "org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat")                                                                                       |
 | :param keyClass: fully qualified classname of key Writable class                                                                                               |
 | (e.g. "org.apache.hadoop.io.IntWritable", None by default)                                                                                                     |
 | :param valueClass: fully qualified classname of value Writable class                                                                                           |
 | (e.g. "org.apache.hadoop.io.Text", None by default)                                                                                                            |
 | :param keyConverter: (None by default)                                                                                                                         |
 | :param valueConverter: (None by default)                                                                                                                       |
 | :param conf: Hadoop job configuration, passed in as a dict (None by default)                                                                                   |
 |                                                                                                                                                                |
 | saveAsPickleFile(self, path, batchSize=10)                                                                                                                     |
 | Save this RDD as a SequenceFile of serialized objects. The serializer                                                                                          |
 | used is L{pyspark.serializers.PickleSerializer}, default batch size                                                                                            |
 | is 10.                                                                                                                                                         |
 |                                                                                                                                                                |
 | >>> tmpFile = NamedTemporaryFile(delete=True)                                                                                                                  |
 | >>> tmpFile.close()                                                                                                                                            |
 | >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)                                                                                   |
 | >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())                                                                                                  |
 | ['1', '2', 'rdd', 'spark']                                                                                                                                     |
 |                                                                                                                                                                |
 | saveAsSequenceFile(self, path, compressionCodecClass=None)                                                                                                     |
 | Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file                                                                             |
 | system, using the L{org.apache.hadoop.io.Writable} types that we convert from the                                                                              |
 | RDD's key and value types. The mechanism is as follows:                                                                                                        |
 |                                                                                                                                                                |
 | 1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.                                                                                    |
 | 2. Keys and values of this Java RDD are converted to Writables and written out.                                                                                |
 |                                                                                                                                                                |
 | :param path: path to sequence file                                                                                                                             |
 | :param compressionCodecClass: (None by default)                                                                                                                |
 |                                                                                                                                                                |
 | saveAsTextFile(self, path, compressionCodecClass=None)                                                                                                         |
 | Save this RDD as a text file, using string representations of elements.                                                                                        |
 |                                                                                                                                                                |
 | @param path: path to text file                                                                                                                                 |
 | @param compressionCodecClass: (None by default) string i.e.                                                                                                    |
 | "org.apache.hadoop.io.compress.GzipCodec"                                                                                                                      |
 |                                                                                                                                                                |
 | >>> tempFile = NamedTemporaryFile(delete=True)                                                                                                                 |
 | >>> tempFile.close()                                                                                                                                           |
 | >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)                                                                                                    |
 | >>> from fileinput import input                                                                                                                                |
 | >>> from glob import glob                                                                                                                                      |
 | >>> ''.join(sorted(input(glob(tempFile.name + "/part-0000*"))))                                                                                                |
 | '0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n'                                                                                                                               |
 |                                                                                                                                                                |
 | Empty lines are tolerated when saving to text files.                                                                                                           |
 |                                                                                                                                                                |
 | >>> tempFile2 = NamedTemporaryFile(delete=True)                                                                                                                |
 | >>> tempFile2.close()                                                                                                                                          |
 | >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)                                                                                  |
 | >>> ''.join(sorted(input(glob(tempFile2.name + "/part-0000*"))))                                                                                               |
 | '\n\n\nbar\nfoo\n'                                                                                                                                             |
 |                                                                                                                                                                |
 | Using compressionCodecClass                                                                                                                                    |
 |                                                                                                                                                                |
 | >>> tempFile3 = NamedTemporaryFile(delete=True)                                                                                                                |
 | >>> tempFile3.close()                                                                                                                                          |
 | >>> codec = "org.apache.hadoop.io.compress.GzipCodec"                                                                                                          |
 | >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)                                                                                       |
 | >>> from fileinput import input, hook_compressed                                                                                                               |
 | >>> result = sorted(input(glob(tempFile3.name + "/part*.gz"), openhook=hook_compressed))                                                                       |
 | >>> b''.join(result).decode('utf-8')                                                                                                                           |
 | u'bar\nfoo\n'                                                                                                                                                  |
 |                                                                                                                                                                |
 | setName(self, name)                                                                                                                                            |
 | Assign a name to this RDD.                                                                                                                                     |
 |                                                                                                                                                                |
 | >>> rdd1 = sc.parallelize([1, 2])                                                                                                                              |
 | >>> rdd1.setName('RDD1').name()                                                                                                                                |
 | u'RDD1'                                                                                                                                                        |
 |                                                                                                                                                                |
 | sortBy(self, keyfunc, ascending=True, numPartitions=None)                                                                                                      |
 | Sorts this RDD by the given keyfunc                                                                                                                            |
 |                                                                                                                                                                |
 | >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]                                                                                                   |
 | >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()                                                                                                       |
 | [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]                                                                                                             |
 | >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()                                                                                                       |
 | [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]                                                                                                             |
 |                                                                                                                                                                |
 | sortByKey(self, ascending=True, numPartitions=None, keyfunc=<function <lambda>>)                                                                               |
 | Sorts this RDD, which is assumed to consist of (key, value) pairs.                                                                                             |
 | # noqa                                                                                                                                                         |
 |                                                                                                                                                                |
 | >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]                                                                                                   |
 | >>> sc.parallelize(tmp).sortByKey().first()                                                                                                                    |
 | ('1', 3)                                                                                                                                                       |
 | >>> sc.parallelize(tmp).sortByKey(True, 1).collect()                                                                                                           |
 | [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]                                                                                                             |
 | >>> sc.parallelize(tmp).sortByKey(True, 2).collect()                                                                                                           |
 | [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]                                                                                                             |
 | >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]                                                                                     |
 | >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])                                                                                       |
 | >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()                                                                             |
 | [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]                                                                               |
 |                                                                                                                                                                |
 | stats(self)                                                                                                                                                    |
 | Return a L{StatCounter} object that captures the mean, variance                                                                                                |
 | and count of the RDD's elements in one operation.                                                                                                              |
 |                                                                                                                                                                |
 | stdev(self)                                                                                                                                                    |
 | Compute the standard deviation of this RDD's elements.                                                                                                         |
 |                                                                                                                                                                |
 | >>> sc.parallelize([1, 2, 3]).stdev()                                                                                                                          |
 | 0.816...                                                                                                                                                       |
 |                                                                                                                                                                |
 | subtract(self, other, numPartitions=None)                                                                                                                      |
 | Return each value in C{self} that is not contained in C{other}.                                                                                                |
 |                                                                                                                                                                |
 | >>> x = sc.parallelize([("a", 1), ("b", 4), ("b", 5), ("a", 3)])                                                                                               |
 | >>> y = sc.parallelize([("a", 3), ("c", None)])                                                                                                                |
 | >>> sorted(x.subtract(y).collect())                                                                                                                            |
 | [('a', 1), ('b', 4), ('b', 5)]                                                                                                                                 |
 |                                                                                                                                                                |
 | subtractByKey(self, other, numPartitions=None)                                                                                                                 |
 | Return each (key, value) pair in C{self} that has no pair with matching                                                                                        |
 | key in C{other}.                                                                                                                                               |
 |                                                                                                                                                                |
 | >>> x = sc.parallelize([("a", 1), ("b", 4), ("b", 5), ("a", 2)])                                                                                               |
 | >>> y = sc.parallelize([("a", 3), ("c", None)])                                                                                                                |
 | >>> sorted(x.subtractByKey(y).collect())                                                                                                                       |
 | [('b', 4), ('b', 5)]                                                                                                                                           |
 |                                                                                                                                                                |
 | sum(self)                                                                                                                                                      |
 | Add up the elements in this RDD.                                                                                                                               |
 |                                                                                                                                                                |
 | >>> sc.parallelize([1.0, 2.0, 3.0]).sum()                                                                                                                      |
 | 6.0                                                                                                                                                            |
 |                                                                                                                                                                |
 | sumApprox(self, timeout, confidence=0.95)                                                                                                                      |
 | .. note:: Experimental                                                                                                                                         |
 |                                                                                                                                                                |
 | Approximate operation to return the sum within a timeout                                                                                                       |
 | or meet the confidence.                                                                                                                                        |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize(range(1000), 10)                                                                                                                      |
 | >>> r = sum(range(1000))                                                                                                                                       |
 | >>> abs(rdd.sumApprox(1000) - r) / r < 0.05                                                                                                                    |
 | True                                                                                                                                                           |
 |                                                                                                                                                                |
 | take(self, num)                                                                                                                                                |
 | Take the first num elements of the RDD.                                                                                                                        |
 |                                                                                                                                                                |
 | It works by first scanning one partition, and use the results from                                                                                             |
 | that partition to estimate the number of additional partitions needed                                                                                          |
 | to satisfy the limit.                                                                                                                                          |
 |                                                                                                                                                                |
 | Note that this method should only be used if the resulting array is expected                                                                                   |
 | to be small, as all the data is loaded into the driver's memory.                                                                                               |
 |                                                                                                                                                                |
 | Translated from the Scala implementation in RDD#take().                                                                                                        |
 |                                                                                                                                                                |
 | >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)                                                                                                            |
 | [2, 3]                                                                                                                                                         |
 | >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)                                                                                                                   |
 | [2, 3, 4, 5, 6]                                                                                                                                                |
 | >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)                                                                                           |
 | [91, 92, 93]                                                                                                                                                   |
 |                                                                                                                                                                |
 | takeOrdered(self, num, key=None)                                                                                                                               |
 | Get the N elements from a RDD ordered in ascending order or as                                                                                                 |
 | specified by the optional key function.                                                                                                                        |
 |                                                                                                                                                                |
 | Note that this method should only be used if the resulting array is expected                                                                                   |
 | to be small, as all the data is loaded into the driver's memory.                                                                                               |
 |                                                                                                                                                                |
 | >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)                                                                                                |
 | [1, 2, 3, 4, 5, 6]                                                                                                                                             |
 | >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)                                                                           |
 | [10, 9, 7, 6, 5, 4]                                                                                                                                            |
 |                                                                                                                                                                |
 | takeSample(self, withReplacement, num, seed=None)                                                                                                              |
 | Return a fixed-size sampled subset of this RDD.                                                                                                                |
 |                                                                                                                                                                |
 | Note that this method should only be used if the resulting array is expected                                                                                   |
 | to be small, as all the data is loaded into the driver's memory.                                                                                               |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize(range(0, 10))                                                                                                                         |
 | >>> len(rdd.takeSample(True, 20, 1))                                                                                                                           |
 | 20                                                                                                                                                             |
 | >>> len(rdd.takeSample(False, 5, 2))                                                                                                                           |
 | 5                                                                                                                                                              |
 | >>> len(rdd.takeSample(False, 15, 3))                                                                                                                          |
 | 10                                                                                                                                                             |
 |                                                                                                                                                                |
 | toDF(self, schema=None, sampleRatio=None)                                                                                                                      |
 | Converts current :class:`RDD` into a :class:`DataFrame`                                                                                                        |
 |                                                                                                                                                                |
 | This is a shorthand for ``spark.createDataFrame(rdd, schema, sampleRatio)``                                                                                    |
 |                                                                                                                                                                |
 | :param schema: a StructType or list of names of columns                                                                                                        |
 | :param samplingRatio: the sample ratio of rows used for inferring                                                                                              |
 | :return: a DataFrame                                                                                                                                           |
 |                                                                                                                                                                |
 | >>> rdd.toDF().collect()                                                                                                                                       |
 | [Row(name=u'Alice', age=1)]                                                                                                                                    |
 |                                                                                                                                                                |
 | toDebugString(self)                                                                                                                                            |
 | A description of this RDD and its recursive dependencies for debugging.                                                                                        |
 |                                                                                                                                                                |
 | toLocalIterator(self)                                                                                                                                          |
 | Return an iterator that contains all of the elements in this RDD.                                                                                              |
 | The iterator will consume as much memory as the largest partition in this RDD.                                                                                 |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize(range(10))                                                                                                                            |
 | >>> [x for x in rdd.toLocalIterator()]                                                                                                                         |
 | [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]                                                                                                                                 |
 |                                                                                                                                                                |
 | top(self, num, key=None)                                                                                                                                       |
 | Get the top N elements from a RDD.                                                                                                                             |
 |                                                                                                                                                                |
 | Note that this method should only be used if the resulting array is expected                                                                                   |
 | to be small, as all the data is loaded into the driver's memory.                                                                                               |
 |                                                                                                                                                                |
 | Note: It returns the list sorted in descending order.                                                                                                          |
 |                                                                                                                                                                |
 | >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)                                                                                                                   |
 | [12]                                                                                                                                                           |
 | >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)                                                                                                                  |
 | [6, 5]                                                                                                                                                         |
 | >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)                                                                                                          |
 | [4, 3, 2]                                                                                                                                                      |
 |                                                                                                                                                                |
 | treeAggregate(self, zeroValue, seqOp, combOp, depth=2)                                                                                                         |
 | Aggregates the elements of this RDD in a multi-level tree                                                                                                      |
 | pattern.                                                                                                                                                       |
 |                                                                                                                                                                |
 | :param depth: suggested depth of the tree (default: 2)                                                                                                         |
 |                                                                                                                                                                |
 | >>> add = lambda x, y: x + y                                                                                                                                   |
 | >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)                                                                                                 |
 | >>> rdd.treeAggregate(0, add, add)                                                                                                                             |
 | -5                                                                                                                                                             |
 | >>> rdd.treeAggregate(0, add, add, 1)                                                                                                                          |
 | -5                                                                                                                                                             |
 | >>> rdd.treeAggregate(0, add, add, 2)                                                                                                                          |
 | -5                                                                                                                                                             |
 | >>> rdd.treeAggregate(0, add, add, 5)                                                                                                                          |
 | -5                                                                                                                                                             |
 | >>> rdd.treeAggregate(0, add, add, 10)                                                                                                                         |
 | -5                                                                                                                                                             |
 |                                                                                                                                                                |
 | treeReduce(self, f, depth=2)                                                                                                                                   |
 | Reduces the elements of this RDD in a multi-level tree pattern.                                                                                                |
 |                                                                                                                                                                |
 | :param depth: suggested depth of the tree (default: 2)                                                                                                         |
 |                                                                                                                                                                |
 | >>> add = lambda x, y: x + y                                                                                                                                   |
 | >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)                                                                                                 |
 | >>> rdd.treeReduce(add)                                                                                                                                        |
 | -5                                                                                                                                                             |
 | >>> rdd.treeReduce(add, 1)                                                                                                                                     |
 | -5                                                                                                                                                             |
 | >>> rdd.treeReduce(add, 2)                                                                                                                                     |
 | -5                                                                                                                                                             |
 | >>> rdd.treeReduce(add, 5)                                                                                                                                     |
 | -5                                                                                                                                                             |
 | >>> rdd.treeReduce(add, 10)                                                                                                                                    |
 | -5                                                                                                                                                             |
 |                                                                                                                                                                |
 | union(self, other)                                                                                                                                             |
 | Return the union of this RDD and another one.                                                                                                                  |
 |                                                                                                                                                                |
 | >>> rdd = sc.parallelize([1, 1, 2, 3])                                                                                                                         |
 | >>> rdd.union(rdd).collect()                                                                                                                                   |
 | [1, 1, 2, 3, 1, 1, 2, 3]                                                                                                                                       |
 |                                                                                                                                                                |
 | unpersist(self)                                                                                                                                                |
 | Mark the RDD as non-persistent, and remove all blocks for it from                                                                                              |
 | memory and disk.                                                                                                                                               |
 |                                                                                                                                                                |
 | values(self)                                                                                                                                                   |
 | Return an RDD with the values of each tuple.                                                                                                                   |
 |                                                                                                                                                                |
 | >>> m = sc.parallelize([(1, 2), (3, 4)]).values()                                                                                                              |
 | >>> m.collect()                                                                                                                                                |
 | [2, 4]                                                                                                                                                         |
 |                                                                                                                                                                |
 | variance(self)                                                                                                                                                 |
 | Compute the variance of this RDD's elements.                                                                                                                   |
 |                                                                                                                                                                |
 | >>> sc.parallelize([1, 2, 3]).variance()                                                                                                                       |
 | 0.666...                                                                                                                                                       |
 |                                                                                                                                                                |
 | zip(self, other)                                                                                                                                               |
 | Zips this RDD with another one, returning key-value pairs with the                                                                                             |
 | first element in each RDD second element in each RDD, etc. Assumes                                                                                             |
 | that the two RDDs have the same number of partitions and the same                                                                                              |
 | number of elements in each partition (e.g. one was made through                                                                                                |
 | a map on the other).                                                                                                                                           |
 |                                                                                                                                                                |
 | >>> x = sc.parallelize(range(0,5))                                                                                                                             |
 | >>> y = sc.parallelize(range(1000, 1005))                                                                                                                      |
 | >>> x.zip(y).collect()                                                                                                                                         |
 | [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]                                                                                                        |
 |                                                                                                                                                                |
 | zipWithIndex(self)                                                                                                                                             |
 | Zips this RDD with its element indices.                                                                                                                        |
 |                                                                                                                                                                |
 | The ordering is first based on the partition index and then the                                                                                                |
 | ordering of items within each partition. So the first item in                                                                                                  |
 | the first partition gets index 0, and the last item in the last                                                                                                |
 | partition receives the largest index.                                                                                                                          |
 |                                                                                                                                                                |
 | This method needs to trigger a spark job when this RDD contains                                                                                                |
 | more than one partitions.                                                                                                                                      |
 |                                                                                                                                                                |
 | >>> sc.parallelize(["a", "b", "c", "d"], 3).zipWithIndex().collect()                                                                                           |
 | [('a', 0), ('b', 1), ('c', 2), ('d', 3)]                                                                                                                       |
 |                                                                                                                                                                |
 | zipWithUniqueId(self)                                                                                                                                          |
 | Zips this RDD with generated unique Long ids.                                                                                                                  |
 |                                                                                                                                                                |
 | Items in the kth partition will get ids k, n+k, 2*n+k, ..., where                                                                                              |
 | n is the number of partitions. So there may exist gaps, but this                                                                                               |
 | method won't trigger a spark job, which is different from                                                                                                      |
 | L{zipWithIndex}                                                                                                                                                |
 |                                                                                                                                                                |
 | >>> sc.parallelize(["a", "b", "c", "d", "e"], 3).zipWithUniqueId().collect()                                                                                   |
 | [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]                                                                                                             |
 |                                                                                                                                                                |
 | ----------------------------------------------------------------------                                                                                         |
 | Data descriptors defined here:                                                                                                                                 |
 |                                                                                                                                                                |
 | __dict__                                                                                                                                                       |
 | dictionary for instance variables (if defined)                                                                                                                 |
 |                                                                                                                                                                |
 | __weakref__                                                                                                                                                    |
 | list of weak references to the object (if defined)                                                                                                             |
 |                                                                                                                                                                |
 | context                                                                                                                                                        |
 | The L{SparkContext} that this RDD was created on.                                                                                                              |

* How to use Window in pyspark?

#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf
from pyspark.sql import Row
sc = SparkContext("local", "my app name")

from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

import numpy as np
np.random.seed(1)

keys = ["foo"] * 10 + ["bar"] * 10
values = np.hstack([np.random.normal(0, 1, 10), np.random.normal(10, 1, 100)])

df = sqlContext.createDataFrame([
   {"k": k, "v": round(float(v), 3)} for k, v in zip(keys, values)])
df.show()

from pyspark.sql.window import Window

w =  Window.partitionBy(df.k).orderBy(df.v)

print w
#+END_SRC

#+RESULTS:
#+begin_example
+---+------+
|  k|     v|
+---+------+
|foo| 1.624|
|foo|-0.612|
|foo|-0.528|
|foo|-1.073|
|foo| 0.865|
|foo|-2.302|
|foo| 1.745|
|foo|-0.761|
|foo| 0.319|
|foo|-0.249|
|bar|11.462|
|bar|  7.94|
|bar| 9.678|
|bar| 9.616|
|bar|11.134|
|bar|   8.9|
|bar| 9.828|
|bar| 9.122|
|bar|10.042|
|bar|10.583|
+---+------+

<pyspark.sql.window.WindowSpec object at 0x7f6d06b4af50>
#+end_example


* Databricks error
java.rmi.RemoteException: com.databricks.s3.S3Exception: org.jets3t.service.S3ServiceException: S3 HEAD request failed for &apos;/AuthorClusterWorkDir-20160121%2FDNG_HPC_item_flatTable&apos; - ResponseCode=403, ResponseMessage=Forbidden; nested exception is: 

This was because I did not enable the databricks role in the notebook.
After enabling this the issue went away.
* Dbutils widget
Db utils is a databricks specific thing.
Databricks utilities
* Learning spark
** How to learn?
https://www.quora.com/Which-book-is-good-to-learn-Spark-and-Scala-for-beginners

The best way to learn is read spark documentation.
https://spark.apache.org/documentation.html


Databricks being one of the committer to spark
https://sparkhub.databricks.com/resources/

And also start experimenting on spark shell (download and install spark ).
 Start with pyspark then move to spark-shell (scala).

To learn scala take coursera scala course 
https://www.coursera.org/learn/progfun1

to know upcoming developments or if you want to contribute go to jira (Spark - ASF JIRA)

https://issues.apache.org/jira/browse/spark/?selectedTab=com.atlassian.jira.jira-projects-plugin%3Asummary-panel


Go through Sameer Farooqui (Databricks) other videos on youtube
https://www.youtube.com/results?search_query=sameer+farooqui


For spark you could follow:
1. Learning Spark: This books is good for spark fundamentals and spark architecture. It explains very efficiently the nuts and bolts behind the working of spark. It also gives a single chapter level introduction to various spark ecosystem frameworks like: Spark SQL, Spark Streaming, MLlib etc.
2. Advanced Analytics with Spark: This book is mainly dedicated to Machine Learning. Each chapter in this book takes a use case and maps it to a Machine Learning algorithm and then solves the use case using that algorithm. This is not a beginners book, it needs basic understanding of Machine Learning and basics of Spark (like RDDs), if you are comfortable with these concepts and want to start with Data Science aspects then this is a great book as of now.

For Scala you could follow:
1. Learning Scala: This is a great book to understand the basics and syntax of scala.
2. Manning: Scala in Action: This book is like a complete reference. It starts from basics and then also covers advance concepts like: database connections, building web applications using scala, interoperability between java and scala etc.


Amit Padwal's reply in above quora link

There are a lot of books available in the market for Spark. But I would suggest reading up a lot of free resources available on the internet before you actually buy a book on Spark.

I have listed a few resources below for your convenience.. Hope this helps you! :)

    5 Things One Must Know About Spark
    Apache Spark vs Hadoop MapReduce
    Your Guide To Career Opportunities In Spark
    Apache Spark & Scala - Edureka Blog
    Spark SQL | Apache Spark
    Spark and Scala Online Training | Spark Certification Course | Edureka

** DONE [#B] Revise spark tutorial
   SCHEDULED: <2017-12-16 Sat 09:30>
   - State "DONE"       from "TODO"       [2017-12-15 Fri 11:31]
   - State "DONE"       from "TODO"       [2017-10-27 Fri 11:06]
   - State "DONE"       from "TODO"       [2017-10-23 Mon 11:17]
   - State "DONE"       from "TODO"       [2017-10-11 Wed 11:09]
   - State "DONE"       from "TODO"       [2017-09-14 Thu 10:28]
   - State "DONE"       from "TODO"       [2017-08-23 Wed 14:14]
   - State "DONE"       from "TODO"       [2017-08-17 Thu 10:07]
   - State "DONE"       from "TODO"       [2017-08-16 Wed 14:35]
   - State "DONE"       from "TODO"       [2017-07-20 Thu 14:37]
   - State "DONE"       from "TODO"       [2017-07-19 Wed 15:18]
   - State "DONE"       from "TODO"       [2017-06-22 Thu 13:21]
   - State "DONE"       from "TODO"       [2017-06-14 Wed 14:53]
   CLOCK: [2017-06-14 Wed 13:56]--[2017-06-14 Wed 14:53] =>  0:57
   - State "DONE"       from "TODO"       [2017-06-09 Fri 11:33]
     CLOCK: [2017-06-09 Fri 11:01]--[2017-06-09 Fri 11:33] =>  0:32

     - State "DONE"       from "TODO"       [2017-06-08 Thu 12:21]
     CLOCK: [2017-06-08 Thu 11:49]--[2017-06-08 Thu 12:21] =>  0:32
     - State "DONE"       from "TODO"       [2017-06-06 Tue 15:20]
     CLOCK: [2017-06-06 Tue 14:55]--[2017-06-06 Tue 15:19] =>  0:24
     - State "DONE"       from "TODO"       [2017-06-01 Thu 12:22]
     CLOCK: [2017-06-01 Thu 11:55]--[2017-06-01 Thu 12:22] =>  0:27
     - State "DONE"       from "TODO"       [2017-05-31 Wed 19:35]
     :PROPERTIES:
   :LAST_REPEAT: [2017-12-15 Fri 11:31]
     :END:
*** DONE Subtask Play with emacs notebook or ipython notebook with same code
    SCHEDULED: <2017-12-22 Mon>
* Notes from databricks tutorial
# distfile = sc.TextFile("abcd.txt", 4)


# rdd = sc.textFile("", 4)
# rdd.filter(iscomment)



# lines = sc.TextFile("", 4)
# lines.cache() saves dont recompute
# comments = lines.filter(iscomment)
# lines.count()
# comments.count()

# no recomputation takes place becaue of caching




** Spark partition RDD creation

#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")

data = list(range(1,10))
rDD = sc.parallelize(data, 4) # 4 partitions
print rDD.collect()


#+END_SRC

#+RESULTS:
: [1, 2, 3, 4, 5, 6, 7, 8, 9]

** Map on a spark RDD


#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")

data = list(range(1,10))
rDD = sc.parallelize(data, 4) # 4 partitions
print rDD.collect()

rdd1 = rDD.map(lambda x: x * 2)
print rdd1.collect()
#+END_SRC

#+RESULTS:
: [1, 2, 3, 4, 5, 6, 7, 8, 9]
: [2, 4, 6, 8, 10, 12, 14, 16, 18]


** Filter on a spark RDD


#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")

data = list(range(1,10))
rDD = sc.parallelize(data, 4) # 4 partitions
print rDD.collect()

even_rdd = rDD.filter(lambda x: x % 2 == 0)
print even_rdd.collect()
#+END_SRC

#+RESULTS:
: [1, 2, 3, 4, 5, 6, 7, 8, 9]
: [2, 4, 6, 8]

** Removing duplicate with distinct
Default number of partitions is probably 1
*** With 4 partitions distinct changes the output order
#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")

data = list(range(1,10) + range(10)) # duplicate the same list twice
rDD = sc.parallelize(data, 4) # 4 partitions
print rDD.collect()

distinct_rdd = rDD.distinct()
print distinct_rdd.collect()
#+END_SRC

#+RESULTS:
: [1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
: [8, 0, 4, 1, 5, 9, 2, 6, 3, 7]


*** With 1 partition distinct gives output in same ascending order
#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")

data = list(range(1,10) + range(10)) # duplicate the same list twice
rDD = sc.parallelize(data)
print rDD.collect()

distinct_rdd = rDD.distinct()
print distinct_rdd.collect()
#+END_SRC

#+RESULTS:
: [1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

** Usage of map like lambda x: [x, x+1]

#+BEGIN_SRC python :results output

from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")

data = list(range(0, 10, 2))
odd_rdd = sc.parallelize(data, 4) # 4 partitions
print odd_rdd.collect()

all_numbers_rdd = odd_rdd.map(lambda x: [x, x + 1])

print all_numbers_rdd.collect()

#+END_SRC

#+RESULTS:
: [0, 2, 4, 6, 8]
: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]


** Usage of flatMap like lambda x: [x, x+1]

#+BEGIN_SRC python :results output

from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")

data = list(range(0, 10, 2))
odd_rdd = sc.parallelize(data, 4) # 4 partitions
print odd_rdd.collect()

all_numbers_rdd = odd_rdd.flatMap(lambda x: [x, x + 1])

print all_numbers_rdd.collect()

#+END_SRC

#+RESULTS:
: [0, 2, 4, 6, 8]
: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

** Usage of reduce

#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")

data = list(range(1, 10 + 1))
rdd = sc.parallelize(data, 4) # 4 partitions
print rdd.collect()

sum_of_rdd = rdd.reduce(lambda a, b: a + b)

print sum_of_rdd
print sum(data)

#+END_SRC

#+RESULTS:
: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
: 55
: 55

** Rdd take elements from take method

#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")

data = list(range(1, 10 + 1))
rdd = sc.parallelize(data, 4) # 4 partitions
print rdd.collect()

taken_from_rdd = rdd.take(2)

print taken_from_rdd

#+END_SRC

#+RESULTS:
: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
: [1, 2]

** Rdd collect elements collect method
#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")

data = list(range(1, 10 + 1))
rdd = sc.parallelize(data, 4) # 4 partitions
print rdd.collect()


#+END_SRC

#+RESULTS:
: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

** Rdd take elements in ordered form takeOrdered method
#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf
import random
sc = SparkContext("local", "my app name")

data = list(range(1, 10 + 1))
random.shuffle(data)
rdd = sc.parallelize(data, 4) # 4 partitions
print rdd.collect()

takeordered_from_rdd = rdd.takeOrdered(5, lambda x: x)
print takeordered_from_rdd
takeordered_from_rdd = rdd.takeOrdered(5, lambda x: -x)
print takeordered_from_rdd
#+END_SRC

#+RESULTS:
: [3, 1, 8, 9, 7, 2, 5, 6, 4, 10]
: [1, 2, 3, 4, 5]
: [10, 9, 8, 7, 6]

   
** Rdd reduce elements by key reduceByKey method
#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf
import random
sc = SparkContext("local", "my app name")

data_list = [(1,2), (3,4), (3, 6), (1, 5), (2,4), (2, 3), (2, 3), (1, 3)]
data_list.sort()
rdd = sc.parallelize(data_list)
print rdd.collect()

reduced_list = rdd.reduceByKey(lambda x, y: x + y).collect()
print reduced_list


#+END_SRC

#+RESULTS:
: [(1, 2), (1, 3), (1, 5), (2, 3), (2, 3), (2, 4), (3, 4), (3, 6)]
: [(1, 10), (2, 10), (3, 10)]

** Rdd sort elements by key sortByKey method
#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf
import random
sc = SparkContext("local", "my app name")

data_list = [(1,2), (3,4), (3, 6), (1, 5), (2,4), (2, 3), (2, 3), (1, 3)]
random.shuffle(data_list)
rdd = sc.parallelize(data_list)
print rdd.collect()

sorted_list = rdd.sortByKey().collect()
print sorted_list

print (sorted(data_list, cmp=lambda x, y: cmp(x, y), key=lambda x: x[0]))

#+END_SRC

#+RESULTS:
: [(2, 3), (1, 2), (1, 5), (3, 4), (2, 3), (1, 3), (3, 6), (2, 4)]
: [(1, 2), (1, 5), (1, 3), (2, 3), (2, 3), (2, 4), (3, 4), (3, 6)]
: [(1, 2), (1, 5), (1, 3), (2, 3), (2, 3), (2, 4), (3, 4), (3, 6)]

** Rdd group elements by key groupByKey method
#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf
import random
sc = SparkContext("local", "my app name")

data_list = [(1,2), (3,4), (3, 6), (1, 5), (2,4), (2, 3), (2, 3), (1, 3)]
random.shuffle(data_list)
rdd = sc.parallelize(data_list)
print rdd.collect()

grouped_list = rdd.groupByKey().collect()

for x, y in grouped_list:
    print "{}: ".format(x),
    print list(y)

#+END_SRC

#+RESULTS:
: [(3, 4), (2, 3), (1, 5), (2, 4), (1, 3), (1, 2), (2, 3), (3, 6)]
: 1:  [5, 3, 2]
: 2:  [3, 4, 3]
: 3:  [4, 6]

** Broadcast variables of spark
#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf
import random
sc = SparkContext("local", "my app name")

broadcastVar = sc.broadcast([1,2,3])
print broadcastVar.value

#+END_SRC

#+RESULTS:
: [1, 2, 3]

** Word count example
I should be expanding on the word count example.
Read the spark tutorial module 2 word count example
*** Writing an example text file for testing
#+BEGIN_SRC python :results output
wordsworth="""My heart leaps up when I behold 
   A rainbow in the sky:
So was it when my life began; 
So is it now I am a man; 
So be it when I shall grow old, 
   Or let me die!
The Child is father of the Man;
And I could wish my days to be
Bound each to each by natural piety.
"""
filename = '/tmp/wordsworth.txt'
with open(filename, 'w') as f:
    f.write(wordsworth)
#+END_SRC

#+RESULTS:


*** Word count example
#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")


def tostrings(str1):
    str2 = str1.strip()
    str_list = str2.split()
    return str_list


def towordcountpair(string1):
    return (string1, 1)

filename = "/tmp/wordsworth.txt"
lines = sc.textFile(filename)

words_list = lines.flatMap(tostrings)
print words_list.collect()

wordcount_pair = words_list.map(towordcountpair)
print wordcount_pair.collect()

final_word_count = wordcount_pair.reduceByKey(lambda a, b : a + b)

print final_word_count.collect()

final_word_count = (lines.flatMap(tostrings).
                    map(towordcountpair).
                    reduceByKey(lambda a, b: a + b))

print final_word_count.collect()

#+END_SRC

#+RESULTS:
: [u'My', u'heart', u'leaps', u'up', u'when', u'I', u'behold', u'A', u'rainbow', u'in', u'the', u'sky:', u'So', u'was', u'it', u'when', u'my', u'life', u'began;', u'So', u'is', u'it', u'now', u'I', u'am', u'a', u'man;', u'So', u'be', u'it', u'when', u'I', u'shall', u'grow', u'old,', u'Or', u'let', u'me', u'die!', u'The', u'Child', u'is', u'father', u'of', u'the', u'Man;', u'And', u'I', u'could', u'wish', u'my', u'days', u'to', u'be', u'Bound', u'each', u'to', u'each', u'by', u'natural', u'piety.']
: [(u'My', 1), (u'heart', 1), (u'leaps', 1), (u'up', 1), (u'when', 1), (u'I', 1), (u'behold', 1), (u'A', 1), (u'rainbow', 1), (u'in', 1), (u'the', 1), (u'sky:', 1), (u'So', 1), (u'was', 1), (u'it', 1), (u'when', 1), (u'my', 1), (u'life', 1), (u'began;', 1), (u'So', 1), (u'is', 1), (u'it', 1), (u'now', 1), (u'I', 1), (u'am', 1), (u'a', 1), (u'man;', 1), (u'So', 1), (u'be', 1), (u'it', 1), (u'when', 1), (u'I', 1), (u'shall', 1), (u'grow', 1), (u'old,', 1), (u'Or', 1), (u'let', 1), (u'me', 1), (u'die!', 1), (u'The', 1), (u'Child', 1), (u'is', 1), (u'father', 1), (u'of', 1), (u'the', 1), (u'Man;', 1), (u'And', 1), (u'I', 1), (u'could', 1), (u'wish', 1), (u'my', 1), (u'days', 1), (u'to', 1), (u'be', 1), (u'Bound', 1), (u'each', 1), (u'to', 1), (u'each', 1), (u'by', 1), (u'natural', 1), (u'piety.', 1)]
: [(u'up', 1), (u'heart', 1), (u'be', 2), (u'when', 3), (u'is', 2), (u'Man;', 1), (u'am', 1), (u'it', 3), (u'Bound', 1), (u'a', 1), (u'in', 1), (u'die!', 1), (u'rainbow', 1), (u'man;', 1), (u'father', 1), (u'Or', 1), (u'to', 2), (u'Child', 1), (u'behold', 1), (u'was', 1), (u'A', 1), (u'piety.', 1), (u'life', 1), (u'shall', 1), (u'I', 4), (u'wish', 1), (u'me', 1), (u'let', 1), (u'grow', 1), (u'The', 1), (u'now', 1), (u'My', 1), (u'by', 1), (u'And', 1), (u'natural', 1), (u'So', 3), (u'of', 1), (u'could', 1), (u'days', 1), (u'began;', 1), (u'sky:', 1), (u'each', 2), (u'leaps', 1), (u'the', 2), (u'my', 2), (u'old,', 1)]
: [(u'up', 1), (u'heart', 1), (u'be', 2), (u'when', 3), (u'is', 2), (u'Man;', 1), (u'am', 1), (u'it', 3), (u'Bound', 1), (u'a', 1), (u'in', 1), (u'die!', 1), (u'rainbow', 1), (u'man;', 1), (u'father', 1), (u'Or', 1), (u'to', 2), (u'Child', 1), (u'behold', 1), (u'was', 1), (u'A', 1), (u'piety.', 1), (u'life', 1), (u'shall', 1), (u'I', 4), (u'wish', 1), (u'me', 1), (u'let', 1), (u'grow', 1), (u'The', 1), (u'now', 1), (u'My', 1), (u'by', 1), (u'And', 1), (u'natural', 1), (u'So', 3), (u'of', 1), (u'could', 1), (u'days', 1), (u'began;', 1), (u'sky:', 1), (u'each', 2), (u'leaps', 1), (u'the', 2), (u'my', 2), (u'old,', 1)]

*** Wordcount all displayed together
#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")


def towordcountpair(string1):
    return (string1, 1)

filename = "/tmp/wordsworth.txt"

final_word_count = (sc.textFile(filename).
                    flatMap(lambda x: x.strip().split()).
                    map(lambda x: (x, 1)).
                    reduceByKey(lambda a, b: a + b))

print final_word_count.collect()

#+END_SRC

#+RESULTS:
: [(u'up', 1), (u'heart', 1), (u'be', 2), (u'when', 3), (u'is', 2), (u'Man;', 1), (u'am', 1), (u'it', 3), (u'Bound', 1), (u'a', 1), (u'in', 1), (u'die!', 1), (u'rainbow', 1), (u'man;', 1), (u'father', 1), (u'Or', 1), (u'to', 2), (u'Child', 1), (u'behold', 1), (u'was', 1), (u'A', 1), (u'piety.', 1), (u'life', 1), (u'shall', 1), (u'I', 4), (u'wish', 1), (u'me', 1), (u'let', 1), (u'grow', 1), (u'The', 1), (u'now', 1), (u'My', 1), (u'by', 1), (u'And', 1), (u'natural', 1), (u'So', 3), (u'of', 1), (u'could', 1), (u'days', 1), (u'began;', 1), (u'sky:', 1), (u'each', 2), (u'leaps', 1), (u'the', 2), (u'my', 2), (u'old,', 1)]

* Spark search
#+BEGIN_SRC sh
binisearch.py "spark"
#+END_SRC

#+RESULTS:
| [[https://www.google.co.in/#q=define:spark][define]]       |          |      |
| [[https://www.google.co.in/#q=spark][base         | search]]   |      |
| [[https://www.google.co.in/#q=intext:spark][intext]]       |          |      |
| [[https://www.google.co.in/#q=][blank]]        |          |      |
| [[https://www.google.co.in/#q=spark%20-][dont         | know]]     |      |
| [[https://www.google.co.in/#q=inurl:spark][inurl]]        |          |      |
| [[https://www.google.co.in/#q=site:.edu%20spark][edu]]          |          |      |
| [[https://www.google.co.in/#q=site:.org%20spark][org]]          |          |      |
| [[https://www.google.co.in/#q=site:.com%20spark][com]]          |          |      |
| [[https://www.google.co.in/#q=site:.in%20spark][in]]           |          |      |
| [[https://www.google.co.in/#q=site:en.wikipedia.org%20spark][wikipedia]]    |          |      |
| [[https://www.google.co.in/#q=filetype:pdf%20spark][pdf]]          |          |      |
| [[https://www.google.co.in/#q=filetype:ppt%20spark][ppt]]          |          |      |
| [[https://www.google.co.in/#q=filetype:doc%20spark][doc]]          |          |      |
| [[https://www.google.co.in/#q=spark%20slide][slide]]        |          |      |
| [[https://www.google.co.in/#q=spark%20glossary][glossary]]     |          |      |
| [[https://www.google.co.in/#q=spark%20faq][faq]]          |          |      |
| [[https://www.google.co.in/#q=spark%20introduction][introduction]] |          |      |
| [[https://www.google.co.in/#q=spark%20books][books]]        |          |      |
| [[https://www.google.co.in/#q=spark%20cheatsheet][cheatsheet]]   |          |      |
| [[https://www.google.co.in/#q=spark%20amazon%20best%20sellers][best         | sellers]]  |      |
| [[https://www.google.co.in/#q=spark%20torrent][torrent]]      |          |      |
| [[https://www.google.co.in/#q=spark%20lecture%20notes][lecture      | notes]]    |      |
| [[https://www.google.co.in/#q=spark%20hashtag%20on%20twitter][twitter]]      |          |      |
| [[https://www.google.co.in/#q=spark%20lectures][lectures]]     |          |      |
| [[https://www.google.co.in/#q=spark%20top%2010][top          | 10]]       |      |
| [[https://www.google.co.in/#q=spark%20alternatives][alternatives]] |          |      |
| [[https://www.google.co.in/#q=spark%20practical][practical]]    |          |      |
| [[https://www.google.co.in/#q=spark%20companies][companies]]    |          |      |
| [[https://www.google.co.in/#q=spark%20government][government]]   |          |      |
| [[https://www.google.co.in/#q=spark%20mind%20the%20book][mind         | the      | book]] |
| [[https://www.google.co.in/#q=spark%20quora][quora]]        |          |      |
| [[https://www.google.co.in/#q=spark%20yahoo%20answers][yahoo        | answers]]  |      |
| [[https://www.google.co.in/#q=spark%20super%20user][super        | user]]     |      |
| [[https://www.google.co.in/#q=spark%20stack%20overflow][stack        | overflow]] |      |
| [[https://www.google.co.in/#q=spark%20arch][arch]]         |          |      |
| [[https://www.google.co.in/#q=spark%20debian][debian]]       |          |      |
| [[https://www.google.co.in/#q=spark%20open%20source][open         | source]]   |      |
| [[https://scholar.google.co.in/scholar?q=define:spark][scholar]]      |          |      |
| [[https://www.google.co.in/search?tbm=vid&q=define:spark][video]]        |          |      |
| [[https://www.google.co.in/search?site=blogsearch&q=define:spark][blogsearch]]   |          |      |
| [[https://www.google.co.in/search?q=define:spark&hs=cX5&source=lnms&tbm=isch&sa=X][image]]        |          |      |
| [[https://www.google.co.in/search?q=define:spark&btnG=Search+Books&tbm=bks&tbo=1][books]]        |          |      |

* Interesting spark stuff to look at

https://discuss.analyticsvidhya.com/t/spark-cheatsheet/14278/2
* Quick cheatsheet

** Loading data
#+BEGIN_SRC python :results output :tangle yes :tangle /tmp/spark_cheatsheet.py
from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")

from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

ls1 = [('a', 7), ('a', 2), ('b', 2)]
ls2 = [('a', 2), ('d', 1), ('b', 1)]
ls3 = [range(100)]
ls4 = [('a', ['x', 'y', 'z']), ('b', ['p', 'r'])]

rdd1 = sc.parallelize(ls1)
rdd2 = sc.parallelize(ls2)
rdd3 = sc.parallelize(ls3)
rdd4 = sc.parallelize(ls4)


#+END_SRC

** FlatmapValues

#+BEGIN_SRC python :results output :tangle yes :tangle /tmp/spark_cheatsheet.py

print (rdd4.
flatMapValues(lambda x: x)
.collect())




#+END_SRC

** Simulating flatmap values with flatmap
#+BEGIN_SRC python :results output :tangle yes :tangle /tmp/spark_cheatsheet.py

print (rdd4.
flatMapValues(lambda x: x)
.collect())


print (rdd4.
map(lambda x: [(x[0], i) for i in x[1]])
.collect())


print (rdd4.
flatMap(lambda x: [(x[0], i) for i in x[1]])
.collect())


#+END_SRC


** iterating over rdd
#+BEGIN_SRC python :results output :tangle yes :tangle /tmp/spark_cheatsheet.py

def g(x):
    print x
# print rdd1
rdd1.foreach(g)
# print help(rdd1)
def f(x): print(x)
sc.parallelize([1, 2, 3, 4, 5]).foreach(f)
  
#+END_SRC

#+RESULTS:


** Executing cheatsheets
#+BEGIN_SRC sh :results output
2>&1 python /tmp/spark_cheatsheet.py
#+END_SRC

#+RESULTS:
#+begin_example
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/08/22 19:15:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/08/22 19:15:05 WARN Utils: Your hostname, ubuntu-16 resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
17/08/22 19:15:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
('a', 7)
('a', 2)
('b', 2)
1
2
3
4
5
[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]
[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]
[[('a', 'x'), ('a', 'y'), ('a', 'z')], [('b', 'p'), ('b', 'r')]]
[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]
#+end_example

* What are the methods in sqlcontext?
#+BEGIN_SRC python :results output
from pyspark import SparkContext, SparkConf

sc = SparkContext("local", "my app name")

from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
print help(sqlContext)
#+END_SRC

#+RESULTS:
#+begin_example
Help on SQLContext in module pyspark.sql.context object:

class SQLContext(__builtin__.object)
 |  The entry point for working with structured data (rows and columns) in Spark, in Spark 1.x.
 |  
 |  As of Spark 2.0, this is replaced by :class:`SparkSession`. However, we are keeping the class
 |  here for backward compatibility.
 |  
 |  A SQLContext can be used create :class:`DataFrame`, register :class:`DataFrame` as
 |  tables, execute SQL over tables, cache tables, and read parquet files.
 |  
 |  :param sparkContext: The :class:`SparkContext` backing this SQLContext.
 |  :param sparkSession: The :class:`SparkSession` around which this SQLContext wraps.
 |  :param jsqlContext: An optional JVM Scala SQLContext. If set, we do not instantiate a new
 |      SQLContext in the JVM, instead we make all calls to this object.
 |  
 |  Methods defined here:
 |  
 |  __init__(self, sparkContext, sparkSession=None, jsqlContext=None)
 |      Creates a new SQLContext.
 |      
 |      >>> from datetime import datetime
 |      >>> sqlContext = SQLContext(sc)
 |      >>> allTypes = sc.parallelize([Row(i=1, s="string", d=1.0, l=1,
 |      ...     b=True, list=[1, 2, 3], dict={"s": 0}, row=Row(a=1),
 |      ...     time=datetime(2014, 8, 1, 14, 1, 5))])
 |      >>> df = allTypes.toDF()
 |      >>> df.createOrReplaceTempView("allTypes")
 |      >>> sqlContext.sql('select i+1, d+1, not b, list[1], dict["s"], time, row.a '
 |      ...            'from allTypes where b and i > 0').collect()
 |      [Row((i + CAST(1 AS BIGINT))=2, (d + CAST(1 AS DOUBLE))=2.0, (NOT b)=False, list[1]=2,             dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]
 |      >>> df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()
 |      [(1, u'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]
 |  
 |  cacheTable(self, tableName)
 |      Caches the specified table in-memory.
 |      
 |      .. versionadded:: 1.0
 |  
 |  clearCache(self)
 |      Removes all cached tables from the in-memory cache.
 |      
 |      .. versionadded:: 1.3
 |  
 |  createDataFrame(self, data, schema=None, samplingRatio=None, verifySchema=True)
 |      Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.
 |      
 |      When ``schema`` is a list of column names, the type of each column
 |      will be inferred from ``data``.
 |      
 |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)
 |      from ``data``, which should be an RDD of :class:`Row`,
 |      or :class:`namedtuple`, or :class:`dict`.
 |      
 |      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string it must match
 |      the real data, or an exception will be thrown at runtime. If the given schema is not
 |      :class:`pyspark.sql.types.StructType`, it will be wrapped into a
 |      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be "value",
 |      each record will also be wrapped into a tuple, which can be converted to row later.
 |      
 |      If schema inference is needed, ``samplingRatio`` is used to determined the ratio of
 |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.
 |      
 |      :param data: an RDD of any kind of SQL data representation(e.g. :class:`Row`,
 |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or
 |          :class:`pandas.DataFrame`.
 |      :param schema: a :class:`pyspark.sql.types.DataType` or a datatype string or a list of
 |          column names, default is None.  The data type string format equals to
 |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can
 |          omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use
 |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.
 |          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.
 |      :param samplingRatio: the sample ratio of rows used for inferring
 |      :param verifySchema: verify data types of every row against schema.
 |      :return: :class:`DataFrame`
 |      
 |      .. versionchanged:: 2.0
 |         The ``schema`` parameter can be a :class:`pyspark.sql.types.DataType` or a
 |         datatype string after 2.0.
 |         If it's not a :class:`pyspark.sql.types.StructType`, it will be wrapped into a
 |         :class:`pyspark.sql.types.StructType` and each record will also be wrapped into a tuple.
 |      
 |      .. versionchanged:: 2.1
 |         Added verifySchema.
 |      
 |      >>> l = [('Alice', 1)]
 |      >>> sqlContext.createDataFrame(l).collect()
 |      [Row(_1=u'Alice', _2=1)]
 |      >>> sqlContext.createDataFrame(l, ['name', 'age']).collect()
 |      [Row(name=u'Alice', age=1)]
 |      
 |      >>> d = [{'name': 'Alice', 'age': 1}]
 |      >>> sqlContext.createDataFrame(d).collect()
 |      [Row(age=1, name=u'Alice')]
 |      
 |      >>> rdd = sc.parallelize(l)
 |      >>> sqlContext.createDataFrame(rdd).collect()
 |      [Row(_1=u'Alice', _2=1)]
 |      >>> df = sqlContext.createDataFrame(rdd, ['name', 'age'])
 |      >>> df.collect()
 |      [Row(name=u'Alice', age=1)]
 |      
 |      >>> from pyspark.sql import Row
 |      >>> Person = Row('name', 'age')
 |      >>> person = rdd.map(lambda r: Person(*r))
 |      >>> df2 = sqlContext.createDataFrame(person)
 |      >>> df2.collect()
 |      [Row(name=u'Alice', age=1)]
 |      
 |      >>> from pyspark.sql.types import *
 |      >>> schema = StructType([
 |      ...    StructField("name", StringType(), True),
 |      ...    StructField("age", IntegerType(), True)])
 |      >>> df3 = sqlContext.createDataFrame(rdd, schema)
 |      >>> df3.collect()
 |      [Row(name=u'Alice', age=1)]
 |      
 |      >>> sqlContext.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP
 |      [Row(name=u'Alice', age=1)]
 |      >>> sqlContext.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP
 |      [Row(0=1, 1=2)]
 |      
 |      >>> sqlContext.createDataFrame(rdd, "a: string, b: int").collect()
 |      [Row(a=u'Alice', b=1)]
 |      >>> rdd = rdd.map(lambda row: row[1])
 |      >>> sqlContext.createDataFrame(rdd, "int").collect()
 |      [Row(value=1)]
 |      >>> sqlContext.createDataFrame(rdd, "boolean").collect() # doctest: +IGNORE_EXCEPTION_DETAIL
 |      Traceback (most recent call last):
 |          ...
 |      Py4JJavaError: ...
 |      
 |      .. versionadded:: 1.3
 |  
 |  createExternalTable(self, tableName, path=None, source=None, schema=None, **options)
 |      Creates an external table based on the dataset in a data source.
 |      
 |      It returns the DataFrame associated with the external table.
 |      
 |      The data source is specified by the ``source`` and a set of ``options``.
 |      If ``source`` is not specified, the default data source configured by
 |      ``spark.sql.sources.default`` will be used.
 |      
 |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and
 |      created external table.
 |      
 |      :return: :class:`DataFrame`
 |      
 |      .. versionadded:: 1.3
 |  
 |  dropTempTable(self, tableName)
 |      Remove the temp table from catalog.
 |      
 |      >>> sqlContext.registerDataFrameAsTable(df, "table1")
 |      >>> sqlContext.dropTempTable("table1")
 |      
 |      .. versionadded:: 1.6
 |  
 |  getConf(self, key, defaultValue=None)
 |      Returns the value of Spark SQL configuration property for the given key.
 |      
 |      If the key is not set and defaultValue is not None, return
 |      defaultValue. If the key is not set and defaultValue is None, return
 |      the system default value.
 |      
 |      >>> sqlContext.getConf("spark.sql.shuffle.partitions")
 |      u'200'
 |      >>> sqlContext.getConf("spark.sql.shuffle.partitions", u"10")
 |      u'10'
 |      >>> sqlContext.setConf("spark.sql.shuffle.partitions", u"50")
 |      >>> sqlContext.getConf("spark.sql.shuffle.partitions", u"10")
 |      u'50'
 |      
 |      .. versionadded:: 1.3
 |  
 |  newSession(self)
 |      Returns a new SQLContext as new session, that has separate SQLConf,
 |      registered temporary views and UDFs, but shared SparkContext and
 |      table cache.
 |      
 |      .. versionadded:: 1.6
 |  
 |  range(self, start, end=None, step=1, numPartitions=None)
 |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named
 |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with
 |      step value ``step``.
 |      
 |      :param start: the start value
 |      :param end: the end value (exclusive)
 |      :param step: the incremental step (default: 1)
 |      :param numPartitions: the number of partitions of the DataFrame
 |      :return: :class:`DataFrame`
 |      
 |      >>> sqlContext.range(1, 7, 2).collect()
 |      [Row(id=1), Row(id=3), Row(id=5)]
 |      
 |      If only one argument is specified, it will be used as the end value.
 |      
 |      >>> sqlContext.range(3).collect()
 |      [Row(id=0), Row(id=1), Row(id=2)]
 |      
 |      .. versionadded:: 1.4
 |  
 |  registerDataFrameAsTable(self, df, tableName)
 |      Registers the given :class:`DataFrame` as a temporary table in the catalog.
 |      
 |      Temporary tables exist only during the lifetime of this instance of :class:`SQLContext`.
 |      
 |      >>> sqlContext.registerDataFrameAsTable(df, "table1")
 |      
 |      .. versionadded:: 1.3
 |  
 |  registerFunction(self, name, f, returnType=StringType)
 |      Registers a python function (including lambda function) as a UDF
 |      so it can be used in SQL statements.
 |      
 |      In addition to a name and the function itself, the return type can be optionally specified.
 |      When the return type is not given it default to a string and conversion will automatically
 |      be done.  For any other return type, the produced object must match the specified type.
 |      
 |      :param name: name of the UDF
 |      :param f: python function
 |      :param returnType: a :class:`pyspark.sql.types.DataType` object
 |      
 |      >>> sqlContext.registerFunction("stringLengthString", lambda x: len(x))
 |      >>> sqlContext.sql("SELECT stringLengthString('test')").collect()
 |      [Row(stringLengthString(test)=u'4')]
 |      
 |      >>> from pyspark.sql.types import IntegerType
 |      >>> sqlContext.registerFunction("stringLengthInt", lambda x: len(x), IntegerType())
 |      >>> sqlContext.sql("SELECT stringLengthInt('test')").collect()
 |      [Row(stringLengthInt(test)=4)]
 |      
 |      >>> from pyspark.sql.types import IntegerType
 |      >>> sqlContext.udf.register("stringLengthInt", lambda x: len(x), IntegerType())
 |      >>> sqlContext.sql("SELECT stringLengthInt('test')").collect()
 |      [Row(stringLengthInt(test)=4)]
 |      
 |      .. versionadded:: 1.2
 |  
 |  registerJavaFunction(self, name, javaClassName, returnType=None)
 |      Register a java UDF so it can be used in SQL statements.
 |      
 |      In addition to a name and the function itself, the return type can be optionally specified.
 |      When the return type is not specified we would infer it via reflection.
 |      :param name:  name of the UDF
 |      :param javaClassName: fully qualified name of java class
 |      :param returnType: a :class:`pyspark.sql.types.DataType` object
 |      
 |      >>> sqlContext.registerJavaFunction("javaStringLength",
 |      ...   "test.org.apache.spark.sql.JavaStringLength", IntegerType())
 |      >>> sqlContext.sql("SELECT javaStringLength('test')").collect()
 |      [Row(UDF(test)=4)]
 |      >>> sqlContext.registerJavaFunction("javaStringLength2",
 |      ...   "test.org.apache.spark.sql.JavaStringLength")
 |      >>> sqlContext.sql("SELECT javaStringLength2('test')").collect()
 |      [Row(UDF(test)=4)]
 |      
 |      .. versionadded:: 2.1
 |  
 |  setConf(self, key, value)
 |      Sets the given Spark SQL configuration property.
 |      
 |      .. versionadded:: 1.3
 |  
 |  sql(self, sqlQuery)
 |      Returns a :class:`DataFrame` representing the result of the given query.
 |      
 |      :return: :class:`DataFrame`
 |      
 |      >>> sqlContext.registerDataFrameAsTable(df, "table1")
 |      >>> df2 = sqlContext.sql("SELECT field1 AS f1, field2 as f2 from table1")
 |      >>> df2.collect()
 |      [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]
 |      
 |      .. versionadded:: 1.0
 |  
 |  table(self, tableName)
 |      Returns the specified table as a :class:`DataFrame`.
 |      
 |      :return: :class:`DataFrame`
 |      
 |      >>> sqlContext.registerDataFrameAsTable(df, "table1")
 |      >>> df2 = sqlContext.table("table1")
 |      >>> sorted(df.collect()) == sorted(df2.collect())
 |      True
 |      
 |      .. versionadded:: 1.0
 |  
 |  tableNames(self, dbName=None)
 |      Returns a list of names of tables in the database ``dbName``.
 |      
 |      :param dbName: string, name of the database to use. Default to the current database.
 |      :return: list of table names, in string
 |      
 |      >>> sqlContext.registerDataFrameAsTable(df, "table1")
 |      >>> "table1" in sqlContext.tableNames()
 |      True
 |      >>> "table1" in sqlContext.tableNames("default")
 |      True
 |      
 |      .. versionadded:: 1.3
 |  
 |  tables(self, dbName=None)
 |      Returns a :class:`DataFrame` containing names of tables in the given database.
 |      
 |      If ``dbName`` is not specified, the current database will be used.
 |      
 |      The returned DataFrame has two columns: ``tableName`` and ``isTemporary``
 |      (a column with :class:`BooleanType` indicating if a table is a temporary one or not).
 |      
 |      :param dbName: string, name of the database to use.
 |      :return: :class:`DataFrame`
 |      
 |      >>> sqlContext.registerDataFrameAsTable(df, "table1")
 |      >>> df2 = sqlContext.tables()
 |      >>> df2.filter("tableName = 'table1'").first()
 |      Row(database=u'', tableName=u'table1', isTemporary=True)
 |      
 |      .. versionadded:: 1.3
 |  
 |  uncacheTable(self, tableName)
 |      Removes the specified table from the in-memory cache.
 |      
 |      .. versionadded:: 1.0
 |  
 |  ----------------------------------------------------------------------
 |  Class methods defined here:
 |  
 |  getOrCreate(cls, sc) from __builtin__.type
 |      Get the existing SQLContext or create a new one with given SparkContext.
 |      
 |      :param sc: SparkContext
 |      
 |      .. versionadded:: 1.6
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors defined here:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)
 |  
 |  read
 |      Returns a :class:`DataFrameReader` that can be used to read data
 |      in as a :class:`DataFrame`.
 |      
 |      :return: :class:`DataFrameReader`
 |      
 |      .. versionadded:: 1.4
 |  
 |  readStream
 |      Returns a :class:`DataStreamReader` that can be used to read data streams
 |      as a streaming :class:`DataFrame`.
 |      
 |      .. note:: Experimental.
 |      
 |      :return: :class:`DataStreamReader`
 |      
 |      >>> text_sdf = sqlContext.readStream.text(tempfile.mkdtemp())
 |      >>> text_sdf.isStreaming
 |      True
 |      
 |      .. versionadded:: 2.0
 |  
 |  streams
 |      Returns a :class:`StreamingQueryManager` that allows managing all the
 |      :class:`StreamingQuery` StreamingQueries active on `this` context.
 |      
 |      .. note:: Experimental.
 |      
 |      .. versionadded:: 2.0
 |  
 |  udf
 |      Returns a :class:`UDFRegistration` for UDF registration.
 |      
 |      :return: :class:`UDFRegistration`
 |      
 |      .. versionadded:: 1.3.1

None
#+end_example

